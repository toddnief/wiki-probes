{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# handle gpu leakage issue on DSI cluster\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tnief/miniconda3/envs/wiki-probes/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "from matplotlib import pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MultiLabelBinarizer, LabelEncoder\n",
    "\n",
    "import torch\n",
    "from torch import Tensor\n",
    "import torch.nn as nn\n",
    "from torch.nn import BCEWithLogitsLoss\n",
    "import torch.nn.functional as F\n",
    "import torch.nn.init as init\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "import transformers\n",
    "from datasets import Dataset\n",
    "from baukit import TraceDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Confirm GPUs are working properly and set default device explicitly\n",
    "torch.cuda.empty_cache()\n",
    "torch.cuda.set_device(0) # Sets the default device for tensors to be the first GPU.\n",
    "device = \"cuda:0\"\n",
    "torch.cuda.device_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This means that tokens that come after special tokens will not be properly handled. We recommend you to read the related pull request available at https://github.com/huggingface/transformers/pull/24565\n",
      "normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.\n",
      "Loading checkpoint shards: 100%|██████████| 3/3 [01:53<00:00, 37.80s/it]\n"
     ]
    }
   ],
   "source": [
    "# MODEL = \"/net/projects/veitch/LLMs/llama2-based-models/llama2-hf/Llama-2-7b-chat-hf\"\n",
    "MODEL = \"/net/projects/veitch/LLMs/llama1-based-models/alpaca-7b\"\n",
    "\n",
    "tokenizer = transformers.LlamaTokenizer.from_pretrained(MODEL)\n",
    "model = transformers.LlamaForCausalLM.from_pretrained(MODEL, low_cpu_mem_usage=True, torch_dtype=torch.float16, device_map=\"auto\").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dictionary for decoding tokens\n",
    "vocab = tokenizer.get_vocab()\n",
    "id_to_token = {id: token for token, id in vocab.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Please say yes only if it costs between 3.22 and 5.76 dollars, otherwise no.\n",
      "\n",
      "### Input:\n",
      "9.30 dollars\n",
      "\n",
      "### Response:\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Any prompt will do to demonstrate. This prompt is 82 tokens long.\n",
    "\n",
    "template = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
    "\n",
    "### Instruction:\n",
    "Please say yes only if it costs between {:.2f} and {:.2f} dollars, otherwise no.\n",
    "\n",
    "### Input:\n",
    "{:.2f} dollars\n",
    "\n",
    "### Response:\n",
    "\"\"\"\n",
    "\n",
    "prompt = template.format(3.22,5.76,9.30)\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just generate some prompts for demonstration\n",
    "# You'd probably save a dataset of prompts, and load those from a file.\n",
    "\n",
    "def generate_prompts(template,n=1000,include_bounds=False):\n",
    "    \"Replicates the same distribution as BDAS paper.\"\n",
    "    for i in range(n):\n",
    "        # Generate the lower bound, upper bound, and input value\n",
    "        lower_bound = np.round(np.random.uniform(0.00,7.49),2)\n",
    "        max_ub = np.min([lower_bound+7.5,9.99])\n",
    "        upper_bound = np.round(np.random.uniform(lower_bound+2.5,max_ub),2)\n",
    "        diff = np.round(upper_bound - lower_bound,2)\n",
    "        assert 2.5 <= diff and diff <= 7.5, (lower_bound, max_ub, upper_bound, diff)\n",
    "        input_value = np.round(np.random.uniform(0.00,9.99),2)\n",
    "\n",
    "        # Generate the prompt\n",
    "        prompt = template.format(lower_bound,upper_bound,input_value)\n",
    "        if include_bounds:\n",
    "            yield (lower_bound,upper_bound,input_value,prompt)\n",
    "        else:\n",
    "            yield prompt\n",
    "\n",
    "prompts = [p for p in generate_prompts(template,n=10)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_activations(prompts,tokenizer,model,device,layer=\"all\"):\n",
    "    \"\"\"Returns a Numpy array of residual stream activations. \n",
    "    Based on https://github.com/likenneth/honest_llama\n",
    "    \n",
    "    David's uncertainties: I think these are the activations before the MLP sublayer?\n",
    "    \"\"\"\n",
    "    tokenized = tokenizer(prompts, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "    input_ids = tokenized.input_ids.to(device)\n",
    "    attention_mask = tokenized.attention_mask.to(device)\n",
    "\n",
    "    model.eval()\n",
    "    outputs = model(\n",
    "        input_ids,\n",
    "        attention_mask=attention_mask, output_hidden_states = True\n",
    "    )\n",
    "    hidden_states = outputs.hidden_states\n",
    "    if layer == \"all\":\n",
    "         # (num_layers, batch_size, seq_length, hidden_dim)\n",
    "        hidden_states = torch.stack(hidden_states, dim = 0).squeeze()\n",
    "        hidden_states = hidden_states.detach().cpu().numpy()\n",
    "    else:\n",
    "         # (batch_size, seq_length, hidden_dim)\n",
    "        hidden_states = hidden_states[layer].detach().cpu().numpy()\n",
    "    return hidden_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 82, 4096)\n"
     ]
    }
   ],
   "source": [
    "# Single prompt, layer 15\n",
    "hidden_states = get_activations(prompts[:1],tokenizer,model,device,layer=15)\n",
    "print(hidden_states.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(33, 82, 4096)\n"
     ]
    }
   ],
   "source": [
    "# Single prompt, all layers. \n",
    "# Note that in this case the shape drops the singluar batch_size dimension. Maybe we should adjust this behavior. But our use case is probing, which is multiple prompts.\n",
    "hidden_states = get_activations(prompts[:1],tokenizer,model,device)\n",
    "print(hidden_states.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 82, 4096)\n"
     ]
    }
   ],
   "source": [
    "# Multiple prompt, layer 15\n",
    "hidden_states = get_activations(prompts,tokenizer,model,device,layer=15)\n",
    "print(hidden_states.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(33, 10, 82, 4096)\n"
     ]
    }
   ],
   "source": [
    "# Multiple prompt, all layers\n",
    "hidden_states = get_activations(prompts,tokenizer,model,device)\n",
    "print(hidden_states.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Some Probes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "LABELS = \"title\" # choices are \"title\" and \"categories\"\n",
    "DATA_FOLDER = \"data/\"\n",
    "TRAIN_DATA = DATA_FOLDER + \"train-10-articles.csv\"\n",
    "VAL_DATA = DATA_FOLDER + \"val-10-articles.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv(TRAIN_DATA)\n",
    "df_val = pd.read_csv(VAL_DATA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_fitted_label_encoder(df, labels=LABELS):\n",
    "    if LABELS == \"categories\":\n",
    "        from ast import literal_eval\n",
    "        unique_labels = set()\n",
    "        for item in df_train['label_list'].tolist():\n",
    "            for sub_item in item:\n",
    "                labels = literal_eval(sub_item)\n",
    "                unique_labels.update(labels)\n",
    "        unique_labels = list(unique_labels)\n",
    "    elif LABELS == \"title\":\n",
    "        unique_labels = list(df_train['title'].drop_duplicates())\n",
    "    \n",
    "    unique_labels = list(unique_labels)\n",
    "    label_encoder = LabelEncoder()\n",
    "    label_encoder.fit(unique_labels)\n",
    "\n",
    "    return label_encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_encoder = get_fitted_label_encoder(df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_to_dataloader(df, model, label_encoder, layer=-1, aggregation=\"max\", labels=LABELS, save=False, filename=DATA_FOLDER + \"dataset.pt\"):\n",
    "    if labels==\"title\":\n",
    "        df['label_encoded'] = label_encoder.transform(df['title'])\n",
    "    elif labels==\"categories\":\n",
    "        encoded_labels = []\n",
    "        for item in df_train['label_list'].tolist():\n",
    "            for sub_item in item:\n",
    "                labels = literal_eval(sub_item)\n",
    "                encoded_labels.append(label_encoder.transform(labels).tolist())\n",
    "        df['label_encoded'] = encoded_labels\n",
    "\n",
    "        def list_to_binary_vector(lst, dim=len(unique_labels)):\n",
    "            return [1 if i in lst else 0 for i in range(dim)]\n",
    "\n",
    "        # Assuming df['label_encoded'] is already a list of integers\n",
    "        df['binary_labels'] = df['label_encoded'].apply(list_to_binary_vector)\n",
    "\n",
    "    Xs = []\n",
    "    ys = []\n",
    "    for i, row in df.iterrows():\n",
    "        hidden_states = get_activations(row.text,tokenizer,model,device)\n",
    "        if aggregation == \"max\":\n",
    "            x = np.max(hidden_states[layer,:,:], axis=0)\n",
    "        elif aggregation == \"mean\":\n",
    "            x = np.mean(hidden_states[layer,:,:], axis=0)\n",
    "        Xs.append(x)\n",
    "        if labels == \"categories\":\n",
    "            ys.append(row.binary_labels)\n",
    "        elif labels == \"title\":\n",
    "            ys.append(row.label_encoded)\n",
    "    \n",
    "    Xs_t = Tensor(np.asarray(Xs)).float()\n",
    "    ys_t = Tensor(np.asarray(ys)).float() if labels == \"categories\" else Tensor(np.asarray(ys)).long()\n",
    "    tensor_dataset = TensorDataset(Xs_t, ys_t)\n",
    "\n",
    "    if save:\n",
    "        torch.save(tensor_dataset, filename)\n",
    "\n",
    "    return DataLoader(tensor_dataset, batch_size=4, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = df_to_dataloader(df_train, model, label_encoder, aggregation=\"max\", layer=-1)\n",
    "val_loader = df_to_dataloader(df_val, model, label_encoder, aggregation=\"max\", layer=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear(nn.Module):\n",
    "    def __init__(self, n_classes=len(label_encoder.classes_)):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(4096, n_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.flatten(x, 1) # flatten all dimensions except batch\n",
    "        x = self.fc1(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, n_classes=len(label_encoder.classes_)):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(4096, 120)\n",
    "        self.fc2 = nn.Linear(120, n_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.flatten(x, 1) # flatten all dimensions except batch\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_weights(m):\n",
    "    if type(m) == nn.Linear:\n",
    "        init.xavier_normal_(m.weight)\n",
    "        init.constant_(m.bias, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Linear(\n",
       "  (fc1): Linear(in_features=4096, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MODEL = \"linear\" # choices are \"linear\" or \"mlp\"\n",
    "if MODEL == \"linear\":\n",
    "    probe = Linear()\n",
    "elif MODEL == \"mlp\":\n",
    "    probe = MLP()\n",
    "probe.apply(init_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "if LABELS == \"categories\":\n",
    "    criterion = BCEWithLogitsLoss(pos_weight=Tensor(torch.ones(len(unique_labels)) * 20))\n",
    "elif LABELS == \"title\":\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# TODO: what optimizer should we actually use\n",
    "# optimizer = optim.AdamW(probe.parameters(), lr=0.001)\n",
    "optimizer = optim.SGD(probe.parameters(), lr=.001, momentum=.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Training][5] loss: 1.045\n",
      "[Validation][5] loss: 2.977\n",
      "[Training][10] loss: 0.002\n",
      "[Validation][10] loss: 0.571\n",
      "[Training][15] loss: 0.000\n",
      "[Validation][15] loss: 0.359\n",
      "[Training][20] loss: 0.000\n",
      "[Validation][20] loss: 0.323\n"
     ]
    }
   ],
   "source": [
    "predicted_labels = 0\n",
    "for epoch in range(EPOCHS):  \n",
    "    probe.train()\n",
    "    train_loss = 0.0\n",
    "    for i, data in enumerate(train_loader):\n",
    "        inputs, labels = data\n",
    "        optimizer.zero_grad()\n",
    "        outputs = probe(inputs)\n",
    "        if torch.all(labels != labels[0]):\n",
    "            print(\"All labels are not the same: \", labels)\n",
    "        if torch.all(outputs != outputs[0]):\n",
    "            print(\"All outputs are not the same: \", outputs)\n",
    "        if LABELS == \"categories\":\n",
    "            pos_labels = labels.sum().item()\n",
    "            neg_labels = labels.numel() - pos_labels\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "\n",
    "    if (epoch + 1) % 5 == 0:\n",
    "        val_loss = 0.0\n",
    "        val_count = 0\n",
    "        predicted_labels = 0\n",
    "        probe.eval()\n",
    "        with torch.no_grad():\n",
    "            for i, data in enumerate(val_loader):  \n",
    "                inputs, labels = data\n",
    "                outputs = probe(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                val_loss += loss.item()\n",
    "                val_count += 1\n",
    "                if LABELS == \"categories\":\n",
    "                    predicted_labels += (torch.sigmoid(outputs) > .5).int().sum().item()\n",
    "\n",
    "        print(f'[Training][{epoch + 1}] loss: {train_loss / len(train_loader):.3f}')\n",
    "        print(f'[Validation][{epoch + 1}] loss: {val_loss / val_count:.3f}')\n",
    "        if LABELS == \"categories\":\n",
    "            print(f'[Predicted Validation Labels]: {predicted_labels}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 5 True\n",
      "6 6 True\n",
      "7 7 True\n",
      "1 1 True\n",
      "5 5 True\n",
      "9 9 True\n",
      "0 0 True\n",
      "1 1 True\n",
      "9 9 True\n",
      "0 0 True\n",
      "8 8 True\n",
      "3 3 True\n",
      "3 3 True\n",
      "4 4 True\n",
      "6 6 True\n",
      "8 8 True\n",
      "3 3 True\n",
      "7 7 True\n",
      "0 0 True\n",
      "4 4 True\n",
      "8 8 True\n",
      "7 7 True\n",
      "2 2 True\n",
      "2 2 True\n",
      "2 2 True\n",
      "9 7 False\n",
      "1 1 True\n",
      "5 5 True\n",
      "7 7 True\n",
      "4 4 True\n",
      "6 3 False\n",
      "Accuracy: 93.55%\n",
      "0: 3 correct\n",
      "1: 3 correct\n",
      "2: 3 correct\n",
      "3: 3 correct\n",
      "4: 3 correct\n",
      "5: 3 correct\n",
      "6: 2 correct\n",
      "7: 4 correct\n",
      "8: 3 correct\n",
      "9: 2 correct\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "# TODO: add something to see which classes get confused\n",
    "\n",
    "correct_total = 0\n",
    "correct_per_class = defaultdict(int)\n",
    "for i, (inputs, labels) in enumerate(val_loader):\n",
    "    preds = torch.max(probe(inputs), dim=1)[1]\n",
    "    for j, lbl in enumerate(labels):\n",
    "        lbl = lbl.item()\n",
    "        correct = lbl == preds[j]\n",
    "        print(lbl, preds[j].item(), correct.item())\n",
    "        correct_per_class[lbl] += correct.item()\n",
    "        correct_total += correct\n",
    "    # print(f\"Batch {i+1}\")\n",
    "    # print(\"Labels:\", labels)\n",
    "    # print(\"Predicted Labels:\", preds)\n",
    "accuracy = (correct_total/len(val_loader.dataset)).item() * 100\n",
    "print(f\"Accuracy: {accuracy:.2f}%\")\n",
    "for k,v in sorted(correct_per_class.items()):\n",
    "    print(f\"{k}: {v} correct\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "iti",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
