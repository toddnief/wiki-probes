{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tnief/miniconda3/envs/wiki-probes/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# from datasets import load_dataset\n",
    "# import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import transformers\n",
    "from baukit import TraceDict\n",
    "from matplotlib import pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "torch.cuda.set_device(0) # Sets the default device for tensors to be the first GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This means that tokens that come after special tokens will not be properly handled. We recommend you to read the related pull request available at https://github.com/huggingface/transformers/pull/24565\n",
      "normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.\n",
      "Loading checkpoint shards: 100%|██████████| 3/3 [01:54<00:00, 38.10s/it]\n"
     ]
    }
   ],
   "source": [
    "# Load the model\n",
    "\n",
    "# MODEL = \"/net/projects/veitch/LLMs/llama2-based-models/llama2-hf/Llama-2-7b-chat-hf\"\n",
    "MODEL = \"/net/projects/veitch/LLMs/llama1-based-models/alpaca-7b\"\n",
    "\n",
    "tokenizer = transformers.LlamaTokenizer.from_pretrained(MODEL)\n",
    "model = transformers.LlamaForCausalLM.from_pretrained(MODEL, low_cpu_mem_usage=True, torch_dtype=torch.float16, device_map=\"auto\")\n",
    "\n",
    "device = \"cuda\"\n",
    "r = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Please say yes only if it costs between 3.22 and 5.76 dollars, otherwise no.\n",
      "\n",
      "### Input:\n",
      "9.30 dollars\n",
      "\n",
      "### Response:\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Any prompt will do to demonstrate. This prompt is 82 tokens long.\n",
    "\n",
    "template = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
    "\n",
    "### Instruction:\n",
    "Please say yes only if it costs between {:.2f} and {:.2f} dollars, otherwise no.\n",
    "\n",
    "### Input:\n",
    "{:.2f} dollars\n",
    "\n",
    "### Response:\n",
    "\"\"\"\n",
    "\n",
    "prompt = template.format(3.22,5.76,9.30)\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just generate some prompts for demonstration\n",
    "# You'd probably save a dataset of prompts, and load those from a file.\n",
    "\n",
    "def generate_prompts(template,n=1000,include_bounds=False):\n",
    "    \"Replicates the same distribution as BDAS paper.\"\n",
    "    for i in range(n):\n",
    "        # Generate the lower bound, upper bound, and input value\n",
    "        lower_bound = np.round(np.random.uniform(0.00,7.49),2)\n",
    "        max_ub = np.min([lower_bound+7.5,9.99])\n",
    "        upper_bound = np.round(np.random.uniform(lower_bound+2.5,max_ub),2)\n",
    "        diff = np.round(upper_bound - lower_bound,2)\n",
    "        assert 2.5 <= diff and diff <= 7.5, (lower_bound, max_ub, upper_bound, diff)\n",
    "        input_value = np.round(np.random.uniform(0.00,9.99),2)\n",
    "\n",
    "        # Generate the prompt\n",
    "        prompt = template.format(lower_bound,upper_bound,input_value)\n",
    "        if include_bounds:\n",
    "            yield (lower_bound,upper_bound,input_value,prompt)\n",
    "        else:\n",
    "            yield prompt\n",
    "\n",
    "prompts = [p for p in generate_prompts(template,n=10)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_activations(prompts,tokenizer,model,device,layer=\"all\"):\n",
    "    \"\"\"Returns a Numpy array of residual stream activations. \n",
    "    Based on https://github.com/likenneth/honest_llama\n",
    "    \n",
    "    David's uncertainties: I think these are the activations before the MLP sublayer?\n",
    "    \"\"\"\n",
    "    # input_ids = tokenizer(prompts, padding=True, truncation=True, return_tensors=\"pt\").input_ids.to(device)\n",
    "    # attention_mask = tokenizer(prompt, return_tensors=\"pt\").attention_mask.to(device)\n",
    "\n",
    "    tokenized = tokenizer(prompts, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "    input_ids = tokenized.input_ids.to(device)\n",
    "    attention_mask = tokenized.attention_mask.to(device)\n",
    "\n",
    "    # print(tokenizer.convert_ids_to_tokens(tokenized[\"input_ids\"][0]))\n",
    "\n",
    "    model.eval()\n",
    "    outputs = model(\n",
    "        input_ids,\n",
    "        attention_mask=attention_mask, output_hidden_states = True\n",
    "    )\n",
    "    hidden_states = outputs.hidden_states\n",
    "    if layer == \"all\":\n",
    "         # (num_layers, batch_size, seq_length, hidden_dim)\n",
    "        hidden_states = torch.stack(hidden_states, dim = 0).squeeze()\n",
    "        hidden_states = hidden_states.detach().cpu().numpy()\n",
    "    else:\n",
    "         # (batch_size, seq_length, hidden_dim)\n",
    "        hidden_states = hidden_states[layer].detach().cpu().numpy()\n",
    "    return hidden_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 82, 4096)\n"
     ]
    }
   ],
   "source": [
    "# Single prompt, layer 15\n",
    "hidden_states = get_activations(prompts[:1],tokenizer,model,device,layer=15)\n",
    "print(hidden_states.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(33, 82, 4096)\n"
     ]
    }
   ],
   "source": [
    "# Single prompt, all layers. \n",
    "# Note that in this case the shape drops the singluar batch_size dimension. Maybe we should adjust this behavior. But our use case is probing, which is multiple prompts.\n",
    "hidden_states = get_activations(prompts[:1],tokenizer,model,device)\n",
    "print(hidden_states.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 82, 4096)\n"
     ]
    }
   ],
   "source": [
    "# Multiple prompt, layer 15\n",
    "hidden_states = get_activations(prompts,tokenizer,model,device,layer=15)\n",
    "print(hidden_states.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(33, 10, 82, 4096)\n"
     ]
    }
   ],
   "source": [
    "# Multiple prompt, all layers\n",
    "hidden_states = get_activations(prompts,tokenizer,model,device)\n",
    "print(hidden_states.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you have a supervised dataset of (prompts,labels), then to train a probe, you'll just replace the prompts with the activations in the learning objective. So you'll train a classifier on (activations,labels)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from torch import Tensor\n",
    "from datasets import Dataset\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from torch.nn import BCEWithLogitsLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv(\"data/parsed-paragraphs-train.csv\")\n",
    "df_test = pd.read_csv(\"data/parsed-paragraphs-test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: hacky way to get unique labels - set up pipeline to actually handle this well\n",
    "\n",
    "from ast import literal_eval\n",
    "\n",
    "unique_labels = set()\n",
    "for item in df_train['label_list'].tolist():\n",
    "    for sub_item in item:\n",
    "        labels = literal_eval(sub_item)\n",
    "        unique_labels.update(labels)\n",
    "\n",
    "unique_labels = list(unique_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[':Anarchism by country', 'Anarchism', 'Anti-capitalism', 'Anti-fascism', 'Economic ideologies', 'Far-left politics', 'Left-wing politics', 'Libertarian socialism', 'Libertarianism', 'Political culture', 'Political ideologies', 'Political movements', 'Social theories', 'Socialism']\n",
      "[':Anarchism by country', 'Anarchism', 'Anti-capitalism', 'Anti-fascism', 'Economic ideologies', 'Far-left politics', 'Left-wing politics', 'Libertarian socialism', 'Libertarianism', 'Political culture', 'Political ideologies', 'Political movements', 'Social theories', 'Socialism']\n",
      "['Land surface effects on climate', 'Climate change feedbacks', 'Climate forcing', 'Climatology', 'Electromagnetic radiation', 'Meteorological quantities', 'Radiometry', 'Scattering, absorption and radiative transfer (optics)', 'Radiation', '1760s neologisms']\n",
      "['Land surface effects on climate', 'Climate change feedbacks', 'Climate forcing', 'Climatology', 'Electromagnetic radiation', 'Meteorological quantities', 'Radiometry', 'Scattering, absorption and radiative transfer (optics)', 'Radiation', '1760s neologisms']\n",
      "['ISO basic Latin letters', 'Vowel letters']\n",
      "['ISO basic Latin letters', 'Vowel letters']\n",
      "['Alabama', '1819 establishments in the United States', 'Southern United States', 'States and territories established in 1819', 'States of the Confederate States of America', 'States of the Gulf Coast of the United States', 'States of the United States', 'Contiguous United States', 'List of place names of Choctaw origin in the United States']\n",
      "['Alabama', '1819 establishments in the United States', 'Southern United States', 'States and territories established in 1819', 'States of the Confederate States of America', 'States of the Gulf Coast of the United States', 'States of the United States', 'Contiguous United States', 'List of place names of Choctaw origin in the United States']\n",
      "['Greek mythological heroes', 'Kings of the Myrmidons', 'Achaean Leaders', 'Thessalians in the Trojan War', 'Metamorphoses characters', 'Mythological rapists', 'Demigods in classical mythology', 'LGBT themes in Greek mythology', 'Achilles', 'Deeds of Apollo', 'Medea', 'Fictional LGBT characters in literature', 'Princes in Greek mythology']\n",
      "['Greek mythological heroes', 'Kings of the Myrmidons', 'Achaean Leaders', 'Thessalians in the Trojan War', 'Metamorphoses characters', 'Mythological rapists', 'Demigods in classical mythology', 'LGBT themes in Greek mythology', 'Achilles', 'Deeds of Apollo', 'Medea', 'Fictional LGBT characters in literature', 'Princes in Greek mythology']\n",
      "['Abraham Lincoln', '1809 births', '1865 deaths', '1865 murders in the United States', '1860s assassinated politicians', '19th-century American politicians', '19th-century presidents of the United States', 'American abolitionists', 'American colonization movement', 'American lawyers admitted to the practice of law by reading law', 'American military personnel of the Indian Wars', 'American militia officers', 'American nationalists', 'American political party founders', 'Illinois postmasters', 'American surveyors', 'Assassinated presidents of the United States', 'Burials at Oak Ridge Cemetery', 'Candidates in the 1860 United States presidential election', 'Candidates in the 1864 United States presidential election', 'Hall of Fame for Great Americans inductees', 'Illinois Central Railroad people', 'Illinois Republicans', 'Illinois lawyers', 'Lincoln family', 'Male murder victims', 'Members of the Illinois House of Representatives', 'People associated with the assassination of Abraham Lincoln', 'People from Coles County, Illinois', 'People from LaRue County, Kentucky', 'People from Macon County, Illinois', 'People from Spencer County, Indiana', 'People murdered in Washington, D.C.', 'People of Illinois in the American Civil War', 'People with mood disorders', 'Politicians from Springfield, Illinois', 'Presidents of the United States', 'Republican Party (United States) presidential nominees', 'Republican Party presidents of the United States', 'Union (American Civil War) political leaders', 'Whig Party members of the United States House of Representatives from Illinois', 'Assassinated heads of state in North America', '19th-century assassinated national presidents']\n",
      "['Abraham Lincoln', '1809 births', '1865 deaths', '1865 murders in the United States', '1860s assassinated politicians', '19th-century American politicians', '19th-century presidents of the United States', 'American abolitionists', 'American colonization movement', 'American lawyers admitted to the practice of law by reading law', 'American military personnel of the Indian Wars', 'American militia officers', 'American nationalists', 'American political party founders', 'Illinois postmasters', 'American surveyors', 'Assassinated presidents of the United States', 'Burials at Oak Ridge Cemetery', 'Candidates in the 1860 United States presidential election', 'Candidates in the 1864 United States presidential election', 'Hall of Fame for Great Americans inductees', 'Illinois Central Railroad people', 'Illinois Republicans', 'Illinois lawyers', 'Lincoln family', 'Male murder victims', 'Members of the Illinois House of Representatives', 'People associated with the assassination of Abraham Lincoln', 'People from Coles County, Illinois', 'People from LaRue County, Kentucky', 'People from Macon County, Illinois', 'People from Spencer County, Indiana', 'People murdered in Washington, D.C.', 'People of Illinois in the American Civil War', 'People with mood disorders', 'Politicians from Springfield, Illinois', 'Presidents of the United States', 'Republican Party (United States) presidential nominees', 'Republican Party presidents of the United States', 'Union (American Civil War) political leaders', 'Whig Party members of the United States House of Representatives from Illinois', 'Assassinated heads of state in North America', '19th-century assassinated national presidents']\n",
      "['Aristotle', 'Aristotelianism', '384 BC births', '322 BC deaths', '4th-century BC mathematicians', '4th-century BC philosophers', '4th-century BC Greek writers', 'Acting theorists', 'Ancient Greek biologists', 'Ancient Greek epistemologists', 'Ancient Greek ethicists', 'Ancient Greek logicians', 'Ancient Greek mathematicians', 'Ancient Greek metaphysicians', 'Ancient Greek philosophers of language', 'Ancient Greek philosophers of mind', 'Ancient Greek physicists', 'Ancient Greek political philosophers', 'Ancient Greek political refugees', 'Ancient Greek philosophers of art', 'Ancient literary critics', 'Ancient Stagirites', 'Aphorists', 'Aristotelian philosophers', 'Attic Greek writers', 'Ancient Greek cosmologists', 'Greek male writers', 'Greek geologists', 'Greek meteorologists', 'Humor researchers', 'Irony theorists', 'Metic philosophers in Classical Athens', 'Natural law ethicists', 'Natural philosophers', 'Ontologists', 'Peripatetic philosophers', 'Philosophers and tutors of Alexander the Great', 'Philosophers of ancient Chalcidice', 'Philosophers of culture', 'Philosophers of education', 'Philosophers of history', 'Philosophers of law', 'Philosophers of literature', 'Philosophers of logic', 'Philosophers of love', 'Philosophers of psychology', 'Philosophers of science', 'Philosophers of time', 'Philosophers of sexuality', 'Philosophers of technology', 'Philosophical logic', 'Philosophical theists', 'Philosophy academics', 'Philosophy writers', 'Rhetoric theorists', 'Social philosophers', 'Students of Plato', 'Trope theorists', 'Virtue ethicists', 'Zoologists']\n",
      "['Aristotle', 'Aristotelianism', '384 BC births', '322 BC deaths', '4th-century BC mathematicians', '4th-century BC philosophers', '4th-century BC Greek writers', 'Acting theorists', 'Ancient Greek biologists', 'Ancient Greek epistemologists', 'Ancient Greek ethicists', 'Ancient Greek logicians', 'Ancient Greek mathematicians', 'Ancient Greek metaphysicians', 'Ancient Greek philosophers of language', 'Ancient Greek philosophers of mind', 'Ancient Greek physicists', 'Ancient Greek political philosophers', 'Ancient Greek political refugees', 'Ancient Greek philosophers of art', 'Ancient literary critics', 'Ancient Stagirites', 'Aphorists', 'Aristotelian philosophers', 'Attic Greek writers', 'Ancient Greek cosmologists', 'Greek male writers', 'Greek geologists', 'Greek meteorologists', 'Humor researchers', 'Irony theorists', 'Metic philosophers in Classical Athens', 'Natural law ethicists', 'Natural philosophers', 'Ontologists', 'Peripatetic philosophers', 'Philosophers and tutors of Alexander the Great', 'Philosophers of ancient Chalcidice', 'Philosophers of culture', 'Philosophers of education', 'Philosophers of history', 'Philosophers of law', 'Philosophers of literature', 'Philosophers of logic', 'Philosophers of love', 'Philosophers of psychology', 'Philosophers of science', 'Philosophers of time', 'Philosophers of sexuality', 'Philosophers of technology', 'Philosophical logic', 'Philosophical theists', 'Philosophy academics', 'Philosophy writers', 'Rhetoric theorists', 'Social philosophers', 'Students of Plato', 'Trope theorists', 'Virtue ethicists', 'Zoologists']\n",
      "['1928 compositions', 'Compositions by George Gershwin', 'Grammy Hall of Fame Award recipients', 'Music about Paris', 'Music commissioned by the New York Philharmonic', 'Symphonic poems']\n",
      "['1928 compositions', 'Compositions by George Gershwin', 'Grammy Hall of Fame Award recipients', 'Music about Paris', 'Music commissioned by the New York Philharmonic', 'Symphonic poems']\n",
      "['Academy Awards', 'Best Art Direction Academy Award winners', 'Awards for best art direction']\n",
      "['Academy Awards', 'Best Art Direction Academy Award winners', 'Awards for best art direction']\n",
      "['Academy Awards', '1929 establishments in California', '1953 American television series debuts', 'Performing arts trophies', 'American annual television specials', 'American film awards', 'Annual events in Los Angeles County, California', 'Awards established in 1929', 'Cinema of Southern California', 'Events in Los Angeles', 'Culture of Hollywood, Los Angeles', 'American live television shows']\n",
      "['Academy Awards', '1929 establishments in California', '1953 American television series debuts', 'Performing arts trophies', 'American annual television specials', 'American film awards', 'Annual events in Los Angeles County, California', 'Awards established in 1929', 'Cinema of Southern California', 'Events in Los Angeles', 'Culture of Hollywood, Los Angeles', 'American live television shows']\n",
      "[\"1986 children's books\", 'Alphabet books', \"Australian children's books\", \"Children's books about animals\", 'Picture books by Graeme Base', 'Puffin Books books', 'Puzzle books']\n",
      "[\"1986 children's books\", 'Alphabet books', \"Australian children's books\", \"Children's books about animals\", 'Picture books by Graeme Base', 'Puffin Books books', 'Puzzle books']\n",
      "['Time scales']\n",
      "['Time scales']\n",
      "['Altruism', 'Auguste Comte', 'Defence mechanisms', 'Morality', 'Moral psychology', 'Philanthropy', 'Social philosophy', 'Interpersonal relationships', 'Virtue']\n",
      "['Altruism', 'Auguste Comte', 'Defence mechanisms', 'Morality', 'Moral psychology', 'Philanthropy', 'Social philosophy', 'Interpersonal relationships', 'Virtue']\n",
      "['Ayn Rand', '1905 births', '1982 deaths', '20th-century American dramatists and playwrights', '20th-century American novelists', '20th-century American philosophers', '20th-century American screenwriters', '20th-century American women writers', '20th-century atheists', '20th-century American essayists', '20th-century pseudonymous writers', '20th-century Russian philosophers', 'Activists from New York (state)', 'American abortion-rights activists', 'American anti-communists', 'American anti-fascists', 'American atheist writers', 'American ethicists', 'American political activists', 'American political philosophers', 'American science fiction writers', 'American secularists', 'American women dramatists and playwrights', 'American women essayists', 'American women novelists', 'American women philosophers', 'American women screenwriters', 'American writers of Russian descent', 'Aristotelian philosophers', 'Atheist philosophers', 'Atheists from the Russian Empire', 'Burials at Kensico Cemetery', 'American critics of Christianity', 'Dramatists and playwrights from the Russian Empire', 'Epistemologists', 'Exophonic writers', 'Female critics of feminism', 'Metaphysicians', 'Novelists from New York (state)', 'Objectivists', 'People with acquired American citizenship', 'Philosophers from New York (state)', 'Political philosophers', 'Pseudonymous women writers', 'Saint Petersburg State University alumni', 'Screenwriters from New York (state)', 'Soviet emigrants to the United States', 'Women science fiction and fantasy writers', 'Writers from New York City', 'Writers from Saint Petersburg']\n",
      "['Ayn Rand', '1905 births', '1982 deaths', '20th-century American dramatists and playwrights', '20th-century American novelists', '20th-century American philosophers', '20th-century American screenwriters', '20th-century American women writers', '20th-century atheists', '20th-century American essayists', '20th-century pseudonymous writers', '20th-century Russian philosophers', 'Activists from New York (state)', 'American abortion-rights activists', 'American anti-communists', 'American anti-fascists', 'American atheist writers', 'American ethicists', 'American political activists', 'American political philosophers', 'American science fiction writers', 'American secularists', 'American women dramatists and playwrights', 'American women essayists', 'American women novelists', 'American women philosophers', 'American women screenwriters', 'American writers of Russian descent', 'Aristotelian philosophers', 'Atheist philosophers', 'Atheists from the Russian Empire', 'Burials at Kensico Cemetery', 'American critics of Christianity', 'Dramatists and playwrights from the Russian Empire', 'Epistemologists', 'Exophonic writers', 'Female critics of feminism', 'Metaphysicians', 'Novelists from New York (state)', 'Objectivists', 'People with acquired American citizenship', 'Philosophers from New York (state)', 'Political philosophers', 'Pseudonymous women writers', 'Saint Petersburg State University alumni', 'Screenwriters from New York (state)', 'Soviet emigrants to the United States', 'Women science fiction and fantasy writers', 'Writers from New York City', 'Writers from Saint Petersburg']\n",
      "['1947 births', 'Living people', 'People from Draguignan', '20th-century French mathematicians', '21st-century French mathematicians', 'Mathematical analysts', 'Differential geometers', 'Fields Medalists', 'Clay Research Award recipients', 'École Normale Supérieure alumni', 'Academic staff of the Collège de France', 'Institute for Advanced Study visiting scholars', 'Foreign associates of the National Academy of Sciences', 'Vanderbilt University faculty', 'Foreign Members of the Russian Academy of Sciences', 'Members of the French Academy of Sciences', 'Members of the Norwegian Academy of Science and Letters', 'Members of the Royal Danish Academy of Sciences and Letters', 'London Mathematical Society']\n",
      "['1947 births', 'Living people', 'People from Draguignan', '20th-century French mathematicians', '21st-century French mathematicians', 'Mathematical analysts', 'Differential geometers', 'Fields Medalists', 'Clay Research Award recipients', 'École Normale Supérieure alumni', 'Academic staff of the Collège de France', 'Institute for Advanced Study visiting scholars', 'Foreign associates of the National Academy of Sciences', 'Vanderbilt University faculty', 'Foreign Members of the Russian Academy of Sciences', 'Members of the French Academy of Sciences', 'Members of the Norwegian Academy of Science and Letters', 'Members of the Royal Danish Academy of Sciences and Letters', 'London Mathematical Society']\n",
      "['1885 births', '1981 deaths', '20th-century American male writers', '20th-century American screenwriters', 'American film directors', 'American film producers', 'American male screenwriters', 'Burials at San Fernando Mission Cemetery', 'Canadian emigrants to the United States', 'Film directors from Toronto', 'Western (genre) film directors', 'Screenwriters from Toronto']\n",
      "['1885 births', '1981 deaths', '20th-century American male writers', '20th-century American screenwriters', 'American film directors', 'American film producers', 'American male screenwriters', 'Burials at San Fernando Mission Cemetery', 'Canadian emigrants to the United States', 'Film directors from Toronto', 'Western (genre) film directors', 'Screenwriters from Toronto']\n",
      "['Algeria', 'North African countries', 'Maghrebi countries', 'Saharan countries', 'Arab republics', 'Republics', 'Countries and territories where Arabic is an official language', 'G15 nations', 'Member states of OPEC', 'Member states of the African Union', 'Member states of the Arab League', 'Member states of the Organisation of Islamic Cooperation', 'Member states of the Group of Friends in Defense of the Charter of the United Nations', 'Member states of the Union for the Mediterranean', 'Member states of the United Nations', 'States and territories established in 1962', '1962 establishments in Algeria', '1962 establishments in Africa', 'Countries in Africa']\n",
      "['Algeria', 'North African countries', 'Maghrebi countries', 'Saharan countries', 'Arab republics', 'Republics', 'Countries and territories where Arabic is an official language', 'G15 nations', 'Member states of OPEC', 'Member states of the African Union', 'Member states of the Arab League', 'Member states of the Organisation of Islamic Cooperation', 'Member states of the Group of Friends in Defense of the Charter of the United Nations', 'Member states of the Union for the Mediterranean', 'Member states of the United Nations', 'States and territories established in 1962', '1962 establishments in Algeria', '1962 establishments in Africa', 'Countries in Africa']\n",
      "['Atlas Shrugged characters', 'Fictional socialites', 'Lists of literary characters', 'Literary characters introduced in 1957']\n",
      "['Atlas Shrugged characters', 'Fictional socialites', 'Lists of literary characters', 'Literary characters introduced in 1957']\n",
      "['Anthropology', 'Behavioural sciences', 'Humans']\n",
      "['Anthropology', 'Behavioural sciences', 'Humans']\n",
      "['Agricultural science']\n",
      "['Agricultural science']\n"
     ]
    }
   ],
   "source": [
    "for item in df_train['label_list'].tolist():\n",
    "    for sub_item in item:\n",
    "        labels = literal_eval(sub_item)\n",
    "        print(labels)\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_to_dataloader(df, save=False):\n",
    "    df['label_list'] = df['label'].apply(lambda x: x.split('|'))\n",
    "    unique_labels = set()\n",
    "    for item in df_train['label_list'].tolist():\n",
    "        for sub_item in item:\n",
    "            labels = literal_eval(sub_item)\n",
    "            unique_labels.update(labels)\n",
    "\n",
    "    unique_labels = list(unique_labels)\n",
    "\n",
    "    label_encoder = LabelEncoder()\n",
    "    label_encoder.fit(unique_labels)\n",
    "\n",
    "    encoded_labels = []\n",
    "    for item in df_train['label_list'].tolist():\n",
    "        for sub_item in item:\n",
    "            labels = literal_eval(sub_item)\n",
    "            encoded_labels.append(label_encoder.transform(labels).tolist())\n",
    "    df['label_encoded'] = encoded_labels\n",
    "\n",
    "    def list_to_binary_vector(lst, dim=len(unique_labels)):\n",
    "        return [1 if i in lst else 0 for i in range(dim)]\n",
    "\n",
    "    # Assuming df['label_encoded'] is already a list of integers\n",
    "    df['binary_labels'] = df['label_encoded'].apply(list_to_binary_vector)\n",
    "\n",
    "    Xs = []\n",
    "    ys = []\n",
    "    for i, row in df.iterrows():\n",
    "        hidden_states = get_activations(row.text,tokenizer,model,device)\n",
    "        Xs.append(hidden_states[:,0,:]) # take only the <s> token\n",
    "        ys.append(row.binary_labels)\n",
    "\n",
    "    tensor_dataset = TensorDataset(Tensor(Xs).float(), Tensor(np.asarray(ys)).float())\n",
    "\n",
    "    if save:\n",
    "        torch.save(tensor_dataset, 'data/example_dataset.pt')\n",
    "\n",
    "    return DataLoader(tensor_dataset, batch_size=4, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = df_to_dataloader(df_train)\n",
    "test_loader = df_to_dataloader(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear(nn.Module):\n",
    "    def __init__(self, n_classes=len(unique_labels)):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(33*4096, n_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.flatten(x, 1) # flatten all dimensions except batch\n",
    "        x = self.fc1(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, n_classes=len(unique_labels)):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(33*4096, 120)\n",
    "        self.fc2 = nn.Linear(120, n_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.flatten(x, 1) # flatten all dimensions except batch\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.init as init\n",
    "\n",
    "def init_weights(m):\n",
    "    if type(m) == nn.Linear:\n",
    "        init.xavier_normal_(m.weight)\n",
    "        init.constant_(m.bias, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FocalLoss(torch.nn.Module):\n",
    "    def __init__(self, alpha=.5, gamma=2):\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        BCE_loss = F.binary_cross_entropy_with_logits(inputs, targets, reduction='none')\n",
    "        pt = torch.exp(-BCE_loss)\n",
    "        F_loss = self.alpha * (1-pt)**self.gamma * BCE_loss\n",
    "        return F_loss.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Linear(\n",
       "  (fc1): Linear(in_features=135168, out_features=294, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linear = Linear()\n",
    "mlp = MLP()\n",
    "mlp.apply(init_weights)\n",
    "linear.apply(init_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "# criterion = BCEWithLogitsLoss(pos_weight=Tensor(torch.ones(len(unique_labels)) * 20))\n",
    "optimizer = optim.AdamW(mlp.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Training][5] loss: 0.657\n",
      "[Validation][5] loss: 0.657\n",
      "[Predicted Validation Labels]: 120\n",
      "[Training][10] loss: 0.646\n",
      "[Validation][10] loss: 0.646\n",
      "[Predicted Validation Labels]: 120\n",
      "[Training][15] loss: 0.636\n",
      "[Validation][15] loss: 0.636\n",
      "[Predicted Validation Labels]: 120\n",
      "[Training][20] loss: 0.624\n",
      "[Validation][20] loss: 0.622\n",
      "[Predicted Validation Labels]: 120\n",
      "[Training][25] loss: 0.613\n",
      "[Validation][25] loss: 0.613\n",
      "[Predicted Validation Labels]: 120\n",
      "[Training][30] loss: 0.602\n",
      "[Validation][30] loss: 0.600\n",
      "[Predicted Validation Labels]: 120\n",
      "[Training][35] loss: 0.590\n",
      "[Validation][35] loss: 0.587\n",
      "[Predicted Validation Labels]: 120\n",
      "[Training][40] loss: 0.579\n",
      "[Validation][40] loss: 0.577\n",
      "[Predicted Validation Labels]: 0\n",
      "[Training][45] loss: 0.567\n",
      "[Validation][45] loss: 0.564\n",
      "[Predicted Validation Labels]: 0\n",
      "[Training][50] loss: 0.555\n",
      "[Validation][50] loss: 0.553\n",
      "[Predicted Validation Labels]: 0\n"
     ]
    }
   ],
   "source": [
    "predicted_labels = 0\n",
    "for epoch in range(50):  \n",
    "    mlp.train()\n",
    "    train_loss = 0.0\n",
    "    for i, data in enumerate(train_loader):\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        inputs, labels = data\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = mlp(inputs)\n",
    "        pos_labels = labels.sum().item()\n",
    "        neg_labels = labels.numel() - pos_labels\n",
    "        criterion = BCEWithLogitsLoss(pos_weight=Tensor(torch.ones(len(unique_labels)) * pos_labels/neg_labels))\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "\n",
    "    if (epoch + 1) % 5 == 0:\n",
    "        val_loss = 0.0\n",
    "        val_count = 0\n",
    "        predicted_labels = 0\n",
    "        mlp.eval()\n",
    "        with torch.no_grad():\n",
    "            for i, data in enumerate(test_loader):  \n",
    "                inputs, labels = data\n",
    "                outputs = mlp(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                val_loss += loss.item()\n",
    "                val_count += 1\n",
    "                predicted_labels += (torch.sigmoid(outputs) > .5).int().sum().item()\n",
    "\n",
    "        print(f'[Training][{epoch + 1}] loss: {train_loss / len(train_loader):.3f}')\n",
    "        print(f'[Validation][{epoch + 1}] loss: {val_loss / val_count:.3f}')\n",
    "        print(f'[Predicted Validation Labels]: {predicted_labels}')\n",
    "    \n",
    "    if predicted_labels > 10000:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 294])"
      ]
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "data, label = next(iter(test_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label[0].int().sum().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "294"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(torch.sigmoid(mlp(data))[0] > .5).int().sum().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "a Tensor with 294 elements cannot be converted to Scalar",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/home/tnief/wiki-probes/get_activations.ipynb Cell 26\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bi001.ds/home/tnief/wiki-probes/get_activations.ipynb#X46sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m (torch\u001b[39m.\u001b[39;49msigmoid(mlp(data))[\u001b[39m0\u001b[39;49m] \u001b[39m>\u001b[39;49m \u001b[39m.5\u001b[39;49m)\u001b[39m.\u001b[39;49mint()\u001b[39m.\u001b[39;49mitem()\n",
      "\u001b[0;31mRuntimeError\u001b[0m: a Tensor with 294 elements cannot be converted to Scalar"
     ]
    }
   ],
   "source": [
    "(torch.sigmoid(mlp(data))[0] > .5).int().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "iti",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
