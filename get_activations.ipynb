{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# handle gpu leakage issue on DSI cluster\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from matplotlib import pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MultiLabelBinarizer, LabelEncoder\n",
    "from ast import literal_eval\n",
    "\n",
    "import torch\n",
    "from torch import Tensor\n",
    "import torch.nn as nn\n",
    "from torch.nn import BCEWithLogitsLoss\n",
    "import torch.nn.functional as F\n",
    "import torch.nn.init as init\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader, Dataset\n",
    "\n",
    "import transformers\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "# from datasets import Dataset\n",
    "from baukit import TraceDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Confirm GPUs are working properly and set default device explicitly\n",
    "torch.cuda.empty_cache()\n",
    "torch.cuda.set_device(0) # Sets the default device for tensors to be the first GPU.\n",
    "device = \"cuda:0\"\n",
    "torch.cuda.device_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This means that tokens that come after special tokens will not be properly handled. We recommend you to read the related pull request available at https://github.com/huggingface/transformers/pull/24565\n",
      "normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "00df8a9a8ab34230b924ea6968c582ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# MODEL = \"/net/projects/veitch/LLMs/llama2-based-models/llama2-hf/Llama-2-7b-chat-hf\"\n",
    "MODEL = \"/net/projects/veitch/LLMs/llama1-based-models/alpaca-7b\"\n",
    "\n",
    "tokenizer = transformers.LlamaTokenizer.from_pretrained(MODEL)\n",
    "model = transformers.LlamaForCausalLM.from_pretrained(MODEL, low_cpu_mem_usage=True, torch_dtype=torch.float16, device_map=\"auto\").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6bf432154bd84c78b055682cb3ffd050",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "(…)face.co/gpt2-xl/resolve/main/config.json:   0%|          | 0.00/689 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "173fa53c8b53436c839b41bb4cd17938",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/6.43G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e638c43b2ac143078ebeea11b1a5ec05",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "(…)2-xl/resolve/main/generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d294438e0e134befa474d5ed0e2dd1ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "(…)gface.co/gpt2-xl/resolve/main/vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa8d39e80ebf4273ac3391a145bec272",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "(…)gface.co/gpt2-xl/resolve/main/merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f572e4c4841c4ee5b0013f9c9ed609bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "(…)e.co/gpt2-xl/resolve/main/tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ROME model and tokenizer\n",
    "MODEL_NAME = \"gpt2-xl\"\n",
    "model_rome = AutoModelForCausalLM.from_pretrained(MODEL_NAME).to(device)\n",
    "tokenizer_rome = AutoTokenizer.from_pretrained(MODEL_NAME),\n",
    "tokenizer_rome.pad_token = tokenizer_rome.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dictionary for decoding tokens\n",
    "vocab = tokenizer.get_vocab()\n",
    "id_to_token = {id: token for token, id in vocab.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Please say yes only if it costs between 3.22 and 5.76 dollars, otherwise no.\n",
      "\n",
      "### Input:\n",
      "9.30 dollars\n",
      "\n",
      "### Response:\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Any prompt will do to demonstrate. This prompt is 82 tokens long.\n",
    "\n",
    "template = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
    "\n",
    "### Instruction:\n",
    "Please say yes only if it costs between {:.2f} and {:.2f} dollars, otherwise no.\n",
    "\n",
    "### Input:\n",
    "{:.2f} dollars\n",
    "\n",
    "### Response:\n",
    "\"\"\"\n",
    "\n",
    "prompt = template.format(3.22,5.76,9.30)\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just generate some prompts for demonstration\n",
    "# You'd probably save a dataset of prompts, and load those from a file.\n",
    "\n",
    "def generate_prompts(template,n=1000,include_bounds=False):\n",
    "    \"Replicates the same distribution as BDAS paper.\"\n",
    "    for i in range(n):\n",
    "        # Generate the lower bound, upper bound, and input value\n",
    "        lower_bound = np.round(np.random.uniform(0.00,7.49),2)\n",
    "        max_ub = np.min([lower_bound+7.5,9.99])\n",
    "        upper_bound = np.round(np.random.uniform(lower_bound+2.5,max_ub),2)\n",
    "        diff = np.round(upper_bound - lower_bound,2)\n",
    "        assert 2.5 <= diff and diff <= 7.5, (lower_bound, max_ub, upper_bound, diff)\n",
    "        input_value = np.round(np.random.uniform(0.00,9.99),2)\n",
    "\n",
    "        # Generate the prompt\n",
    "        prompt = template.format(lower_bound,upper_bound,input_value)\n",
    "        if include_bounds:\n",
    "            yield (lower_bound,upper_bound,input_value,prompt)\n",
    "        else:\n",
    "            yield prompt\n",
    "\n",
    "prompts = [p for p in generate_prompts(template,n=10)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_activations(prompts,tokenizer,model,device,layer=\"all\"):\n",
    "    \"\"\"Returns a Numpy array of residual stream activations. \n",
    "    Based on https://github.com/likenneth/honest_llama\n",
    "    \n",
    "    David's uncertainties: I think these are the activations before the MLP sublayer?\n",
    "    \"\"\"\n",
    "    tokenized = tokenizer(prompts, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "    input_ids = tokenized.input_ids.to(device)\n",
    "    attention_mask = tokenized.attention_mask.to(device)\n",
    "\n",
    "    model.eval()\n",
    "    outputs = model(\n",
    "        input_ids,\n",
    "        attention_mask=attention_mask, output_hidden_states = True\n",
    "    )\n",
    "    hidden_states = outputs.hidden_states\n",
    "    if layer == \"all\":\n",
    "         # (num_layers, batch_size, seq_length, hidden_dim)\n",
    "        hidden_states = torch.stack(hidden_states, dim = 0).squeeze()\n",
    "        hidden_states = hidden_states.detach().cpu().numpy()\n",
    "    else:\n",
    "         # (batch_size, seq_length, hidden_dim)\n",
    "        hidden_states = hidden_states[layer].detach().cpu().numpy()\n",
    "    return hidden_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 69, 1600)\n"
     ]
    }
   ],
   "source": [
    "# Single prompt, layer 15\n",
    "hidden_states = get_activations(prompts[:1],tokenizer_rome,model_rome,device,layer=15)\n",
    "print(hidden_states.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(49, 69, 1600)\n"
     ]
    }
   ],
   "source": [
    "# Single prompt, all layers. \n",
    "# Note that in this case the shape drops the singluar batch_size dimension. Maybe we should adjust this behavior. But our use case is probing, which is multiple prompts.\n",
    "hidden_states = get_activations(prompts[:1],tokenizer_rome,model_rome,device)\n",
    "print(hidden_states.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 82, 4096)\n"
     ]
    }
   ],
   "source": [
    "# Multiple prompt, layer 15\n",
    "hidden_states = get_activations(prompts,tokenizer,model,device,layer=15)\n",
    "print(hidden_states.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(33, 10, 82, 4096)\n"
     ]
    }
   ],
   "source": [
    "# Multiple prompt, all layers\n",
    "hidden_states = get_activations(prompts,tokenizer,model,device)\n",
    "print(hidden_states.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Some Probes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "LABELS = \"title\" # choices are \"title\" and \"categories\"\n",
    "DATA_FOLDER = \"data/\"\n",
    "TRAIN_DATA = DATA_FOLDER + \"train-10-articles.csv\"\n",
    "VAL_DATA = DATA_FOLDER + \"val-10-articles.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv(TRAIN_DATA)\n",
    "df_val = pd.read_csv(VAL_DATA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_fitted_label_encoder(df, labels=LABELS):\n",
    "    if labels == \"categories\":\n",
    "        from ast import literal_eval\n",
    "        unique_labels = set()\n",
    "        for item in df['categories'].tolist():\n",
    "            categories = literal_eval(item)\n",
    "            for cat in categories:\n",
    "                unique_labels.update([cat])\n",
    "        unique_labels = list(unique_labels)\n",
    "    elif labels == \"title\":\n",
    "        unique_labels = list(df['title'].drop_duplicates())\n",
    "    \n",
    "    label_encoder = LabelEncoder()\n",
    "    label_encoder.fit(unique_labels)\n",
    "\n",
    "    return label_encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Agriculture', 'Alaska', 'Alchemy', 'Algae', 'Amphibian',\n",
       "       'Anarchism', 'Animation', 'Anthropology', 'Appellate court',\n",
       "       'Astronomer'], dtype='<U15')"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_encoder_title = get_fitted_label_encoder(df_train, labels=\"title\")\n",
    "label_encoder_title.classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['1959 establishments in the United States',\n",
       "       ':Anarchism by country', 'Agriculture', 'Agronomy', 'Alaska',\n",
       "       'Alchemy', 'Algae', 'Amphibians', 'Amphibious organisms',\n",
       "       'Anarchism', 'Animation', 'Anthropology', 'Anti-capitalism',\n",
       "       'Anti-fascism', 'Appellate courts', 'Arctic Ocean',\n",
       "       'Articles containing video clips', 'Astronomers', 'Astronomy',\n",
       "       'Behavioural sciences', 'Beringia', 'Cartooning',\n",
       "       'Common names of organisms', 'Courts by type',\n",
       "       'Economic ideologies', 'Enclaves and exclaves',\n",
       "       'Endosymbiotic events', 'Esotericism',\n",
       "       'Exclaves in the United States',\n",
       "       'Extant Late Devonian first appearances', 'Far-left politics',\n",
       "       'Film and video technology', 'Food industry',\n",
       "       'Former Russian colonies', 'Hermeticism', 'History of science',\n",
       "       'Humans', 'Jurisdiction', 'Left-wing politics',\n",
       "       'Libertarian socialism', 'Libertarianism', 'Natural philosophy',\n",
       "       'Northern America', 'Political culture', 'Political ideologies',\n",
       "       'Political movements', 'Polyphyletic groups',\n",
       "       'Russia–United States relations', 'Science occupations',\n",
       "       'Social theories', 'Socialism',\n",
       "       'States and territories established in 1959',\n",
       "       'States of the United States',\n",
       "       'States of the West Coast of the United States',\n",
       "       'Taxa named by John Edward Gray', 'Western United States'],\n",
       "      dtype='<U45')"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_encoder_cats = get_fitted_label_encoder(df_train, labels=\"categories\")\n",
    "label_encoder_cats.classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActivationsDataset(Dataset):\n",
    "    def __init__(self, Xs, title, categories, text):\n",
    "        self.Xs = Xs\n",
    "        self.title = title\n",
    "        self.categories = categories\n",
    "        self.text = text\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.Xs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.Xs[idx], self.title[idx], self.categories[idx], self.text[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_categories(cat_list, label_encoder):\n",
    "    encoded_categories = []\n",
    "    for cat in cat_list:\n",
    "        encoded_cat = label_encoder.transform(literal_eval(cat)).tolist()\n",
    "        encoded_categories.append(encoded_cat)\n",
    "    return encoded_categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_to_dataset(df, model, label_encoder_title, label_encoder_cats, layer=-1, aggregation=\"max\", labels=LABELS, save=False, filename=DATA_FOLDER + \"dataset.pt\"):\n",
    "    # TODO: this is kinda ugly\n",
    "    # title\n",
    "    df['title_encoded'] = label_encoder_title.transform(df['title'])\n",
    "\n",
    "    # categories\n",
    "    df['label_encoded'] = parse_categories(df['categories'].tolist(), label_encoder_cats)\n",
    "    def list_to_binary_vector(lst, dim=len(label_encoder_cats.classes_)):\n",
    "        return [1 if i in lst else 0 for i in range(dim)]\n",
    "    df['binary_labels'] = df['label_encoded'].apply(list_to_binary_vector)\n",
    "\n",
    "    Xs = []\n",
    "    titles = []\n",
    "    categories = []\n",
    "    text = []\n",
    "    for i, row in df.iterrows():\n",
    "        hidden_states = get_activations(row.text,tokenizer,model,device)\n",
    "        if aggregation == \"max\":\n",
    "            x = np.max(hidden_states[layer,:,:], axis=0)\n",
    "        elif aggregation == \"mean\":\n",
    "            x = np.mean(hidden_states[layer,:,:], axis=0)\n",
    "        Xs.append(x)\n",
    "        titles.append(row.title_encoded)\n",
    "        categories.append(row.binary_labels)\n",
    "        text.append(row.text)\n",
    "    \n",
    "    Xs_t = Tensor(np.asarray(Xs)).float()\n",
    "    titles_t = Tensor(np.asarray(titles)).long() # cross entropy loss wants a long dtype\n",
    "    categories_t = Tensor(np.asarray(categories)).float() # binary cross entropy loss wants a float dtype\n",
    "\n",
    "    return ActivationsDataset(Xs_t, titles_t, categories_t, text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "LAYER = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = df_to_dataset(df_train, model, label_encoder_title, label_encoder_cats, aggregation=\"max\", layer=LAYER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dataset = df_to_dataset(df_val, model, label_encoder_title, label_encoder_cats, aggregation=\"max\", layer=LAYER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_loader = DataLoader(val_dataset, batch_size=4, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear(nn.Module):\n",
    "    # TODO: figure out better way to handle the number of classes for default\n",
    "    def __init__(self, hidden_size=model.config.hidden_size, n_classes=len(label_encoder_title.classes_)):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(hidden_size, n_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.flatten(x, 1) # flatten all dimensions except batch\n",
    "        x = self.fc1(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, hidden_size=model.config.hidden_size, n_classes=len(label_encoder_title.classes_)):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(hidden_size, 120)\n",
    "        self.fc2 = nn.Linear(120, n_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.flatten(x, 1) # flatten all dimensions except batch\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_weights(m):\n",
    "    if type(m) == nn.Linear:\n",
    "        init.xavier_normal_(m.weight)\n",
    "        init.constant_(m.bias, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Linear(\n",
       "  (fc1): Linear(in_features=4096, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MODEL = \"linear\" # choices are \"linear\" or \"mlp\"\n",
    "if MODEL == \"linear\":\n",
    "    probe = Linear()\n",
    "elif MODEL == \"mlp\":\n",
    "    probe = MLP()\n",
    "probe.apply(init_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "if LABELS == \"categories\":\n",
    "    criterion = BCEWithLogitsLoss(pos_weight=Tensor(torch.ones(len(label_encoder_cats.classes_)) * 20))\n",
    "elif LABELS == \"title\":\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# TODO: what optimizer should we actually use?\n",
    "optimizer = optim.AdamW(probe.parameters(), lr=0.001)\n",
    "# optimizer = optim.SGD(probe.parameters(), lr=.001, momentum=.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Training][5] loss: 0.045\n",
      "[Validation][5] loss: 1.010\n",
      "[Validation]5 accuracy: 0.933\n",
      "[Training][10] loss: 0.097\n",
      "[Validation][10] loss: 2.447\n",
      "[Validation]10 accuracy: 0.833\n",
      "[Training][15] loss: 4.238\n",
      "[Validation][15] loss: 8.288\n",
      "[Validation]15 accuracy: 0.800\n",
      "[Training][20] loss: 0.013\n",
      "[Validation][20] loss: 0.356\n",
      "[Validation]20 accuracy: 0.933\n",
      "[Training][25] loss: 0.000\n",
      "[Validation][25] loss: 0.126\n",
      "[Validation]25 accuracy: 0.967\n",
      "[Training][30] loss: 0.000\n",
      "[Validation][30] loss: 0.139\n",
      "[Validation]30 accuracy: 0.967\n",
      "[Training][35] loss: 0.000\n",
      "[Validation][35] loss: 0.147\n",
      "[Validation]35 accuracy: 0.967\n",
      "[Training][40] loss: 0.000\n",
      "[Validation][40] loss: 0.154\n",
      "[Validation]40 accuracy: 0.967\n",
      "[Training][45] loss: 0.000\n",
      "[Validation][45] loss: 0.160\n",
      "[Validation]45 accuracy: 0.967\n",
      "[Training][50] loss: 0.000\n",
      "[Validation][50] loss: 0.163\n",
      "[Validation]50 accuracy: 0.967\n",
      "[Training][55] loss: 0.000\n",
      "[Validation][55] loss: 0.163\n",
      "[Validation]55 accuracy: 0.967\n",
      "[Training][60] loss: 0.000\n",
      "[Validation][60] loss: 0.156\n",
      "[Validation]60 accuracy: 0.967\n",
      "[Training][65] loss: 0.000\n",
      "[Validation][65] loss: 0.145\n",
      "[Validation]65 accuracy: 0.967\n",
      "[Training][70] loss: 0.000\n",
      "[Validation][70] loss: 0.131\n",
      "[Validation]70 accuracy: 0.967\n",
      "[Training][75] loss: 0.000\n",
      "[Validation][75] loss: 0.116\n",
      "[Validation]75 accuracy: 0.967\n",
      "[Training][80] loss: 0.000\n",
      "[Validation][80] loss: 0.096\n",
      "[Validation]80 accuracy: 0.967\n",
      "[Training][85] loss: 0.000\n",
      "[Validation][85] loss: 0.076\n",
      "[Validation]85 accuracy: 0.967\n",
      "[Training][90] loss: 0.000\n",
      "[Validation][90] loss: 0.060\n",
      "[Validation]90 accuracy: 0.967\n",
      "[Training][95] loss: 0.000\n",
      "[Validation][95] loss: 0.050\n",
      "[Validation]95 accuracy: 0.967\n",
      "[Training][100] loss: 0.000\n",
      "[Validation][100] loss: 0.048\n",
      "[Validation]100 accuracy: 0.967\n",
      "[Training][105] loss: 0.000\n",
      "[Validation][105] loss: 0.047\n",
      "[Validation]105 accuracy: 0.967\n",
      "[Training][110] loss: 36.362\n",
      "[Validation][110] loss: 49.562\n",
      "[Validation]110 accuracy: 0.333\n",
      "[Training][115] loss: 0.082\n",
      "[Validation][115] loss: 4.049\n",
      "[Validation]115 accuracy: 0.800\n",
      "[Training][120] loss: 122.455\n",
      "[Validation][120] loss: 312.188\n",
      "[Validation]120 accuracy: 0.100\n",
      "[Training][125] loss: 0.084\n",
      "[Validation][125] loss: 0.068\n",
      "[Validation]125 accuracy: 0.967\n",
      "[Training][130] loss: 0.107\n",
      "[Validation][130] loss: 0.186\n",
      "[Validation]130 accuracy: 0.967\n",
      "[Training][135] loss: 59.334\n",
      "[Validation][135] loss: 459.352\n",
      "[Validation]135 accuracy: 0.100\n",
      "[Training][140] loss: 0.059\n",
      "[Validation][140] loss: 0.623\n",
      "[Validation]140 accuracy: 0.967\n",
      "[Training][145] loss: 0.000\n",
      "[Validation][145] loss: 0.203\n",
      "[Validation]145 accuracy: 0.967\n",
      "[Training][150] loss: 0.000\n",
      "[Validation][150] loss: 0.175\n",
      "[Validation]150 accuracy: 0.967\n",
      "[Training][155] loss: 0.000\n",
      "[Validation][155] loss: 0.152\n",
      "[Validation]155 accuracy: 0.967\n",
      "[Training][160] loss: 0.000\n",
      "[Validation][160] loss: 0.131\n",
      "[Validation]160 accuracy: 0.967\n",
      "[Training][165] loss: 0.000\n",
      "[Validation][165] loss: 0.111\n",
      "[Validation]165 accuracy: 0.967\n",
      "[Training][170] loss: 0.000\n",
      "[Validation][170] loss: 0.093\n",
      "[Validation]170 accuracy: 0.967\n",
      "[Training][175] loss: 0.000\n",
      "[Validation][175] loss: 0.076\n",
      "[Validation]175 accuracy: 0.967\n",
      "[Training][180] loss: 0.000\n",
      "[Validation][180] loss: 0.061\n",
      "[Validation]180 accuracy: 0.967\n",
      "[Training][185] loss: 0.000\n",
      "[Validation][185] loss: 0.047\n",
      "[Validation]185 accuracy: 0.967\n",
      "[Training][190] loss: 0.000\n",
      "[Validation][190] loss: 0.035\n",
      "[Validation]190 accuracy: 0.967\n",
      "[Training][195] loss: 0.000\n",
      "[Validation][195] loss: 0.026\n",
      "[Validation]195 accuracy: 0.967\n",
      "text:  cold-blooded vertebrates (vertebrates have backbones) that don’t have scales. They live part of their lives in water and part on land.\n",
      "labels:  Amphibian\n",
      "predicted labels:  Amphibian\n",
      "text:  the broad term for everything that goes into growing crops and raising animals, to provide food and materials that people can use and enjoy\n",
      "labels:  Agriculture\n",
      "predicted labels:  Agriculture\n",
      "text:  scientists who study the Universe and the objects within it\n",
      "labels:  Astronomer\n",
      "predicted labels:  Astronomer\n",
      "text:  the study of human biological and physiological characteristics and their evolution\n",
      "labels:  Anthropology\n",
      "predicted labels:  Anthropology\n",
      "text:  Persons or entities such as corporations that experience an unsuccessful outcome in a trial-level or other lower courts may file an appeal with an appellate court to have the decision reviewed\n",
      "labels:  Appellate court\n",
      "predicted labels:  Appellate court\n",
      "text:  a diverse group of aquatic organisms that have the ability to conduct photosynthesis\n",
      "labels:  Algae\n",
      "predicted labels:  Algae\n",
      "text:  small vertebrates that need water, or a moist environment, to survive. The species in this group include frogs, toads, salamanders, and newts. All can breathe and absorb water through their very thin skin.\n",
      "labels:  Amphibian\n",
      "predicted labels:  Amphibian\n",
      "text:  the manipulation of electronic images by means of a computer in order to create moving images.\n",
      "labels:  Animation\n",
      "predicted labels:  Animation\n",
      "text:  the preparation of animated cartoons\n",
      "labels:  Animation\n",
      "predicted labels:  Animation\n",
      "text:  a simple, nonflowering, and typically aquatic plant of a large group that includes the seaweeds and many single-celled forms. Algae contain chlorophyll but lack true stems, roots, leaves, and vascular tissue.\n",
      "labels:  Algae\n",
      "predicted labels:  Algae\n",
      "text:  or farming, is the simplification of nature's food webs and the rechanneling of energy for human planting and animal consumption\n",
      "labels:  Agriculture\n",
      "predicted labels:  Agriculture\n",
      "text:  the part of the American judicial system that is responsible for hearing and reviewing appeals from legal cases that have already been heard in a trial-level or other lower court\n",
      "labels:  Appellate court\n",
      "predicted labels:  Appellate court\n",
      "text:  the medieval forerunner of chemistry, based on the supposed transformation of matter. It was concerned particularly with attempts to convert base metals into gold or to find a universal elixir.\n",
      "labels:  Alchemy\n",
      "predicted labels:  Alchemy\n",
      "text:  a medieval chemical science and speculative philosophy aiming to achieve the transmutation of the base metals into gold, the discovery of a universal cure for disease, and the discovery of a means of indefinitely prolonging life\n",
      "labels:  Alchemy\n",
      "predicted labels:  Alchemy\n",
      "text:  the study of human societies and cultures and their development\n",
      "labels:  Anthropology\n",
      "predicted labels:  Anthropology\n",
      "text:  studies celestial objects and phenomena in the universe. They explore and investigate various aspects of the cosmos, including stars, planets, galaxies, asteroids, comets, and other celestial bodies\n",
      "labels:  Astronomer\n",
      "predicted labels:  Astronomer\n",
      "text:  the organization of society on the basis of voluntary cooperation, without political institutions or hierarchical government\n",
      "labels:  Anarchism\n",
      "predicted labels:  Anarchism\n",
      "text:  the largest state in the union and; one-fifth the size of the lower 48 states\n",
      "labels:  Alaska\n",
      "predicted labels:  Alaska\n",
      "text:  members of a group of predominantly aquatic photosynthetic organisms of the kingdom Protista\n",
      "labels:  Algae\n",
      "predicted labels:  Algae\n",
      "text:  the largest state owned by the United States of America and is part of the beautiful North American continent.\n",
      "labels:  Alaska\n",
      "predicted labels:  Alaska\n",
      "text:  the technique of photographing successive drawings or positions of puppets or models to create an illusion of movement when the movie is shown as a sequence.\n",
      "labels:  Animation\n",
      "predicted labels:  Animation\n",
      "text:  a form of speculative thought that, among other aims, tried to transform base metals such as lead or copper into silver or gold and to discover a cure for disease and a way of extending life\n",
      "labels:  Alchemy\n",
      "predicted labels:  Alchemy\n",
      "text:  a cold-blooded vertebrate animal of a class that comprises the frogs, toads, newts, and salamanders. They are distinguished by having an aquatic gill-breathing larval stage followed (typically) by a terrestrial lung-breathing adult stage.\n",
      "labels:  Amphibian\n",
      "predicted labels:  Amphibian\n",
      "text:  review the procedures and the decisions in the trial court to make sure that the proceedings were fair and that the proper law was applied correctly\n",
      "labels:  Appellate court\n",
      "predicted labels:  Appellate court\n",
      "text:  a utopian society of individuals who enjoy complete freedom without government\n",
      "labels:  Anarchism\n",
      "predicted labels:  Anarchism\n",
      "text:  one of two US states not bordered by another state; Hawaii is the other.\n",
      "labels:  Alaska\n",
      "predicted labels:  Alaska\n",
      "text:  a state of disorder due to absence or nonrecognition of authority or other controlling systems\n",
      "labels:  Anarchism\n",
      "predicted labels:  Anarchism\n",
      "text:  They use telescopes, both on the ground and in space, to observe and collect data from distant objects. They analyze the light emitted or reflected by celestial bodies to determine their properties, such as their composition, temperature, distance, and motion\n",
      "labels:  Astronomer\n",
      "predicted labels:  Astronomer\n",
      "text:  theology dealing with the origin, nature, and destiny of human beings\n",
      "labels:  Anthropology\n",
      "predicted labels:  Anthropology\n",
      "text:  the science or practice of farming, including cultivation of the soil for the growing of crops and the rearing of animals to provide food, wool, and other products.\n",
      "labels:  Agriculture\n",
      "predicted labels:  Agriculture\n",
      "[Training][200] loss: 0.000\n",
      "[Validation][200] loss: 0.018\n",
      "[Validation]200 accuracy: 1.000\n"
     ]
    }
   ],
   "source": [
    "predicted_labels = 0\n",
    "for epoch in range(EPOCHS):  \n",
    "    probe.train()\n",
    "    train_loss = 0.0\n",
    "    for i, data in enumerate(train_loader):\n",
    "        # TODO: this is sloppy\n",
    "        inputs, titles, categories, text = data\n",
    "        labels = titles if LABELS == \"title\" else categories\n",
    "        optimizer.zero_grad()\n",
    "        outputs = probe(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "\n",
    "        if LABELS == \"title\":\n",
    "            _, predicted_labels = torch.max(outputs, 1)\n",
    "        elif LABELS == \"categories\":\n",
    "            predicted_labels = (torch.sigmoid(outputs) > .5).int()\n",
    "            total += labels.numel()\n",
    "\n",
    "        # for txt, lbl, pred in zip(text, label_encoder_title.inverse_transform(labels), label_encoder_title.inverse_transform(predicted_labels)):\n",
    "        #     print(\"text: \", txt)\n",
    "        #     print(\"labels: \", lbl)\n",
    "        #     print(\"predicted labels: \", pred)\n",
    "        #     total += labels[0]\n",
    "\n",
    "    if (epoch + 1) % 5 == 0:\n",
    "        val_loss = 0.0\n",
    "        total = 0\n",
    "        correct = 0\n",
    "        incorrect_examples = []\n",
    "        probe.eval()\n",
    "        with torch.no_grad():\n",
    "            for i, data in enumerate(val_loader):  \n",
    "                inputs, titles, categories, text = data\n",
    "                labels = titles if LABELS == \"title\" else categories\n",
    "                outputs = probe(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                val_loss += loss.item()\n",
    "                if LABELS == \"title\":\n",
    "                    _, predicted_labels = torch.max(outputs, 1)\n",
    "                elif LABELS == \"categories\":\n",
    "                    predicted_labels = (torch.sigmoid(outputs) > .5).int()\n",
    "                    total += labels.numel()\n",
    "                graded_preds = predicted_labels == labels\n",
    "                # if (~graded_preds).sum() > 0:\n",
    "                #     incorrect_examples.append(text)\n",
    "                correct += (graded_preds).sum().item()\n",
    "                # TODO: set this up to be the correct encoder depending on label\n",
    "                if epoch + 1 == EPOCHS:\n",
    "                    for txt, lbl, pred in zip(text, label_encoder_title.inverse_transform(labels), label_encoder_title.inverse_transform(predicted_labels)):\n",
    "                        print(\"text: \", txt)\n",
    "                        print(\"labels: \", lbl)\n",
    "                        print(\"predicted labels: \", pred)\n",
    "                total += labels.size()[0]\n",
    "\n",
    "        print(f'[Training][{epoch + 1}] loss: {train_loss / len(train_loader):.3f}')\n",
    "        print(f'[Validation][{epoch + 1}] loss: {val_loss / len(val_loader):.3f}') #TODO: I broke this somehow for title\n",
    "        print(f'[Validation]{epoch + 1} accuracy: {correct / total:.3f}')\n",
    "# print(\"Incorrect examples on last val step: \", incorrect_examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "iti",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
