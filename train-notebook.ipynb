{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# handle gpu leakage issue on DSI cluster\n",
    "# ideally we shouldn't have to do this but leaving it for now in case the issue pops up again\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Optional, Tuple, Union\n",
    "\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, LlamaForCausalLM, LlamaTokenizer\n",
    "from transformers.modeling_outputs import CausalLMOutputWithPast\n",
    "\n",
    "import scripts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Confirm GPUs are working properly and set default device explicitly\n",
    "device = \"cuda:0\"\n",
    "torch.cuda.device_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "## References ##\n",
    "# Huggingface docs: https://huggingface.co/docs/transformers/main/model_doc/llama#transformers.LlamaForCausalLM \n",
    "\n",
    "## modeling_llama.py ##\n",
    "# LlamaForCausalLM » https://github.com/huggingface/transformers/blob/main/src/transformers/models/llama/modeling_llama.py#L989 \n",
    "# forward » https://github.com/huggingface/transformers/blob/main/src/transformers/models/llama/modeling_llama.py#L989 \n",
    "\n",
    "## utils.py ##\n",
    "# GenerationMixin » https://github.com/huggingface/transformers/blob/main/src/transformers/generation/utils.py#L571\n",
    "# generate » https://github.com/huggingface/transformers/blob/main/src/transformers/generation/utils.py#L1432\n",
    "# greedy_search » https://github.com/huggingface/transformers/blob/main/src/transformers/generation/utils.py#L2447 \n",
    "# actual forward pass » https://github.com/huggingface/transformers/blob/main/src/transformers/generation/utils.py#L2614 \n",
    "\n",
    "class EditableLlama(LlamaForCausalLM):\n",
    "    def __init__(self, config, *args, **kwargs):\n",
    "        super().__init__(config, *args, **kwargs)\n",
    "        self.edit_weights = None\n",
    "        self.edit_bias = None\n",
    "        self.edit_scalar = None\n",
    "        self.edit_count = 0\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids: torch.LongTensor = None,\n",
    "        attention_mask: Optional[torch.Tensor] = None,\n",
    "        position_ids: Optional[torch.LongTensor] = None,\n",
    "        past_key_values: Optional[List[torch.FloatTensor]] = None,\n",
    "        inputs_embeds: Optional[torch.FloatTensor] = None,\n",
    "        labels: Optional[torch.LongTensor] = None,\n",
    "        use_cache: Optional[bool] = None,\n",
    "        output_attentions: Optional[bool] = None,\n",
    "        output_hidden_states: Optional[bool] = None,\n",
    "        return_dict: Optional[bool] = None,\n",
    "    ) -> Union[Tuple, CausalLMOutputWithPast]:\n",
    "        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n",
    "        output_hidden_states = (\n",
    "            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n",
    "        )\n",
    "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
    "\n",
    "        # decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\n",
    "        outputs = self.model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            position_ids=position_ids,\n",
    "            past_key_values=past_key_values,\n",
    "            inputs_embeds=inputs_embeds,\n",
    "            use_cache=use_cache,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "            return_dict=return_dict,\n",
    "        )\n",
    "\n",
    "        hidden_states = outputs[0]\n",
    "\n",
    "        # TODO: maybe this should actually take the model dtype if I can get it\n",
    "        # Also, should I edit every token?\n",
    "        # torch.Size([1, 11, 4096])\n",
    "        # torch.Size([1, 1, 4096])\n",
    "        if self.edit is not None:\n",
    "            edit_norm = self.edit / torch.norm(self.edit)\n",
    "            # TODO: wait...do I want the final token or the first token?\n",
    "            projection = edit_norm * torch.dot(hidden_states[:,-1,:].squeeze(0), edit_norm)\n",
    "            print(projection.shape)\n",
    "            hidden_states = hidden_states - projection\n",
    "            print(hidden_states.shape)\n",
    "            # TODO: renormalize?\n",
    "\n",
    "        if self.config.pretraining_tp > 1:\n",
    "            lm_head_slices = self.lm_head.weight.split(self.vocab_size // self.config.pretraining_tp, dim=0)\n",
    "            logits = [F.linear(hidden_states, lm_head_slices[i]) for i in range(self.config.pretraining_tp)]\n",
    "            logits = torch.cat(logits, dim=-1)\n",
    "        else:\n",
    "            logits = self.lm_head(hidden_states)\n",
    "        logits = logits.float()\n",
    "\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            # Shift so that tokens < n predict n\n",
    "            shift_logits = logits[..., :-1, :].contiguous()\n",
    "            shift_labels = labels[..., 1:].contiguous()\n",
    "            # Flatten the tokens\n",
    "            loss_fct = CrossEntropyLoss()\n",
    "            shift_logits = shift_logits.view(-1, self.config.vocab_size)\n",
    "            shift_labels = shift_labels.view(-1)\n",
    "            # Enable model parallelism\n",
    "            shift_labels = shift_labels.to(shift_logits.device)\n",
    "            loss = loss_fct(shift_logits, shift_labels)\n",
    "\n",
    "        if not return_dict:\n",
    "            output = (logits,) + outputs[1:]\n",
    "            return (loss,) + output if loss is not None else output\n",
    "\n",
    "        return CausalLMOutputWithPast(\n",
    "            loss=loss,\n",
    "            logits=logits,\n",
    "            past_key_values=outputs.past_key_values,\n",
    "            hidden_states=outputs.hidden_states,\n",
    "            attentions=outputs.attentions,\n",
    "        )\n",
    "\n",
    "    def generate_with_edit(self, inputs, edit_weights=None, edit_bias=None, edit_scalar=None, **generate_kwargs):\n",
    "        self.edit_weights = edit_weights\n",
    "        self.edit_bias = edit_bias\n",
    "        self.edit_scalar = edit_scalar\n",
    "        self.edit_count = 0\n",
    "        return self.generate(inputs, **generate_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_DIR = \"/net/projects/veitch/LLMs/\"\n",
    "MODEL = \"llama1-based-models/alpaca-7b\" # or llama2-based-models/llama2-hf/Llama-2-7b-chat-hf\n",
    "MODEL_PATH = MODEL_DIR + MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Clean this up\n",
    "# This cell is for doing ROME edits\n",
    "# ROME model and tokenizer\n",
    "# MODEL_NAME = \"gpt2-xl\"\n",
    "# model = AutoModelForCausalLM.from_pretrained(MODEL_NAME).to(device)\n",
    "# tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "# tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = LlamaTokenizer.from_pretrained(MODEL_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb0c15d9299f4f1a9e23fea81d731b2d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# model = LlamaForCausalLM.from_pretrained(MODEL_PATH, low_cpu_mem_usage=True, torch_dtype=torch.float16, device_map=\"auto\").to(device)\n",
    "model = EditableLlama.from_pretrained(MODEL_PATH, low_cpu_mem_usage=True, torch_dtype=torch.float16, device_map=\"auto\").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "LABELS = \"title\" # choices are \"title\" and \"categories\"\n",
    "DATA_FOLDER = \"data/\"\n",
    "TRAIN_FILEPATH = DATA_FOLDER + \"train-12-articles.csv\"\n",
    "VAL_FILEPATH = DATA_FOLDER + \"val-12-articles.csv\"\n",
    "PROBE_TYPE = \"linear\" # choices are \"linear\" and \"mlp\"\n",
    "LAYER = None\n",
    "EPOCHS = 50\n",
    "BATCH_SIZE = 4\n",
    "AGGREGATION = \"max\" # choices are \"max\" or \"mean\"\n",
    "PRINT_PROGRESS = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, label_encoder_title, label_encoder_cats = scripts.get_hidden_states(TRAIN_FILEPATH, model, tokenizer, device)\n",
    "val_data, _, _ = scripts.get_hidden_states(VAL_FILEPATH, model, tokenizer, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to use entire flattened activations use layer = None\n",
    "train_dataset = scripts.create_dataset(*train_data, layer=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dataset = scripts.create_dataset(*val_data, layer=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4096])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_dataset.Xs[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Training][5] loss: 1.400\n",
      "[Validation][5] loss: 0.055\n",
      "[Validation]5 accuracy: 0.917\n",
      "[Training][10] loss: 1.884\n",
      "[Validation][10] loss: 2.103\n",
      "[Validation]10 accuracy: 0.861\n",
      "[Training][15] loss: 0.464\n",
      "[Validation][15] loss: 0.280\n",
      "[Validation]15 accuracy: 0.917\n",
      "[Training][20] loss: 0.000\n",
      "[Validation][20] loss: 0.184\n",
      "[Validation]20 accuracy: 0.917\n",
      "[Training][25] loss: 0.000\n",
      "[Validation][25] loss: 0.167\n",
      "[Validation]25 accuracy: 0.917\n",
      "[Training][30] loss: 0.000\n",
      "[Validation][30] loss: 0.158\n",
      "[Validation]30 accuracy: 0.917\n",
      "[Training][35] loss: 0.000\n",
      "[Validation][35] loss: 0.146\n",
      "[Validation]35 accuracy: 0.917\n",
      "[Training][40] loss: 0.000\n",
      "[Validation][40] loss: 0.148\n",
      "[Validation]40 accuracy: 0.917\n",
      "[Training][45] loss: 0.000\n",
      "[Validation][45] loss: 0.147\n",
      "[Validation]45 accuracy: 0.917\n",
      "text:  the technique of photographing successive drawings or positions of puppets or models to create an illusion of movement when the movie is shown as a sequence.\n",
      "labels:  Animation\n",
      "predicted labels:  Animation\n",
      "text:  an American game that is played between two teams of 11 players each and in which the ball is moved forward by running or passing\n",
      "labels:  American football\n",
      "predicted labels:  Basketball\n",
      "text:  a simple, nonflowering, and typically aquatic plant of a large group that includes the seaweeds and many single-celled forms. Algae contain chlorophyll but lack true stems, roots, leaves, and vascular tissue.\n",
      "labels:  Algae\n",
      "predicted labels:  Algae\n",
      "text:  a game that is played by two teams of eleven players using an oval-shaped ball. Players try to score points by passing or carrying the ball to their opponents' end of the field, or by kicking it over a bar fixed between two posts.\n",
      "labels:  American football\n",
      "predicted labels:  Basketball\n",
      "text:  the preparation of animated cartoons\n",
      "labels:  Animation\n",
      "predicted labels:  Animation\n",
      "text:  theology dealing with the origin, nature, and destiny of human beings\n",
      "labels:  Anthropology\n",
      "predicted labels:  Anthropology\n",
      "text:  or farming, is the simplification of nature's food webs and the rechanneling of energy for human planting and animal consumption\n",
      "labels:  Agriculture\n",
      "predicted labels:  Agriculture\n",
      "text:  the study of human societies and cultures and their development\n",
      "labels:  Anthropology\n",
      "predicted labels:  Anthropology\n",
      "text:  small vertebrates that need water, or a moist environment, to survive. The species in this group include frogs, toads, salamanders, and newts. All can breathe and absorb water through their very thin skin.\n",
      "labels:  Amphibian\n",
      "predicted labels:  Amphibian\n",
      "text:  a medieval chemical science and speculative philosophy aiming to achieve the transmutation of the base metals into gold, the discovery of a universal cure for disease, and the discovery of a means of indefinitely prolonging life\n",
      "labels:  Alchemy\n",
      "predicted labels:  Alchemy\n",
      "text:  scientists who study the Universe and the objects within it\n",
      "labels:  Astronomer\n",
      "predicted labels:  Astronomer\n",
      "text:  studies celestial objects and phenomena in the universe. They explore and investigate various aspects of the cosmos, including stars, planets, galaxies, asteroids, comets, and other celestial bodies\n",
      "labels:  Astronomer\n",
      "predicted labels:  Astronomer\n",
      "text:  the broad term for everything that goes into growing crops and raising animals, to provide food and materials that people can use and enjoy\n",
      "labels:  Agriculture\n",
      "predicted labels:  Agriculture\n",
      "text:  cold-blooded vertebrates (vertebrates have backbones) that don’t have scales. They live part of their lives in water and part on land.\n",
      "labels:  Amphibian\n",
      "predicted labels:  Amphibian\n",
      "text:  a utopian society of individuals who enjoy complete freedom without government\n",
      "labels:  Anarchism\n",
      "predicted labels:  Anarchism\n",
      "text:  a state of disorder due to absence or nonrecognition of authority or other controlling systems\n",
      "labels:  Anarchism\n",
      "predicted labels:  Anarchism\n",
      "text:  the science or practice of farming, including cultivation of the soil for the growing of crops and the rearing of animals to provide food, wool, and other products.\n",
      "labels:  Agriculture\n",
      "predicted labels:  Agriculture\n",
      "text:  a game played between two teams of five players in which goals are scored by throwing a ball through a netted hoop fixed above each end of the court\n",
      "labels:  Basketball\n",
      "predicted labels:  Basketball\n",
      "text:  the largest state in the union and; one-fifth the size of the lower 48 states\n",
      "labels:  Alaska\n",
      "predicted labels:  Alaska\n",
      "text:  the study of human biological and physiological characteristics and their evolution\n",
      "labels:  Anthropology\n",
      "predicted labels:  Anthropology\n",
      "text:  Persons or entities such as corporations that experience an unsuccessful outcome in a trial-level or other lower courts may file an appeal with an appellate court to have the decision reviewed\n",
      "labels:  Appellate court\n",
      "predicted labels:  Appellate court\n",
      "text:  the medieval forerunner of chemistry, based on the supposed transformation of matter. It was concerned particularly with attempts to convert base metals into gold or to find a universal elixir.\n",
      "labels:  Alchemy\n",
      "predicted labels:  Alchemy\n",
      "text:  the largest state owned by the United States of America and is part of the beautiful North American continent.\n",
      "labels:  Alaska\n",
      "predicted labels:  Alaska\n",
      "text:  review the procedures and the decisions in the trial court to make sure that the proceedings were fair and that the proper law was applied correctly\n",
      "labels:  Appellate court\n",
      "predicted labels:  Appellate court\n",
      "text:  a diverse group of aquatic organisms that have the ability to conduct photosynthesis\n",
      "labels:  Algae\n",
      "predicted labels:  Algae\n",
      "text:  a usually indoor court game between two teams of usually five players each who score by tossing an inflated ball through a raised goal\n",
      "labels:  Basketball\n",
      "predicted labels:  Basketball\n",
      "text:  a form of team game played in North America with an oval ball on a field marked out as a gridiron\n",
      "labels:  American football\n",
      "predicted labels:  Basketball\n",
      "text:  the manipulation of electronic images by means of a computer in order to create moving images.\n",
      "labels:  Animation\n",
      "predicted labels:  Animation\n",
      "text:  the part of the American judicial system that is responsible for hearing and reviewing appeals from legal cases that have already been heard in a trial-level or other lower court\n",
      "labels:  Appellate court\n",
      "predicted labels:  Appellate court\n",
      "text:  the organization of society on the basis of voluntary cooperation, without political institutions or hierarchical government\n",
      "labels:  Anarchism\n",
      "predicted labels:  Anarchism\n",
      "text:  a cold-blooded vertebrate animal of a class that comprises the frogs, toads, newts, and salamanders. They are distinguished by having an aquatic gill-breathing larval stage followed (typically) by a terrestrial lung-breathing adult stage.\n",
      "labels:  Amphibian\n",
      "predicted labels:  Amphibian\n",
      "text:  They use telescopes, both on the ground and in space, to observe and collect data from distant objects. They analyze the light emitted or reflected by celestial bodies to determine their properties, such as their composition, temperature, distance, and motion\n",
      "labels:  Astronomer\n",
      "predicted labels:  Astronomer\n",
      "text:  The objective of the game is to throw (shoot) a ball through the top of a circular band (referred to as the rim) that has cord hanging around its circumference (with both being named the basket), which is itself attached to a backboard\n",
      "labels:  Basketball\n",
      "predicted labels:  Basketball\n",
      "text:  a form of speculative thought that, among other aims, tried to transform base metals such as lead or copper into silver or gold and to discover a cure for disease and a way of extending life\n",
      "labels:  Alchemy\n",
      "predicted labels:  Alchemy\n",
      "text:  one of two US states not bordered by another state; Hawaii is the other.\n",
      "labels:  Alaska\n",
      "predicted labels:  Alaska\n",
      "text:  members of a group of predominantly aquatic photosynthetic organisms of the kingdom Protista\n",
      "labels:  Algae\n",
      "predicted labels:  Algae\n",
      "[Training][50] loss: 0.000\n",
      "[Validation][50] loss: 0.114\n",
      "[Validation]50 accuracy: 0.917\n"
     ]
    }
   ],
   "source": [
    "probe, _ = scripts.train_handler(train_dataset, val_dataset, label_encoder_title, probe_type=PROBE_TYPE, labels=LABELS, batch_size=BATCH_SIZE, epochs=EPOCHS, print_progress=PRINT_PROGRESS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try Editing With Linear Probe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([12, 4096])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probe.fc1.weight.data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([-0.0066,  0.0050, -0.0160,  0.0071, -0.0330, -0.0091, -0.0022,  0.0082,\n",
       "         0.0081,  0.0164,  0.0045,  0.0345], requires_grad=True)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probe.fc1.bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(\n",
    "        self,\n",
    "        input_ids: torch.LongTensor = None,\n",
    "        attention_mask: Optional[torch.Tensor] = None,\n",
    "        position_ids: Optional[torch.LongTensor] = None,\n",
    "        past_key_values: Optional[List[torch.FloatTensor]] = None,\n",
    "        inputs_embeds: Optional[torch.FloatTensor] = None,\n",
    "        labels: Optional[torch.LongTensor] = None,\n",
    "        use_cache: Optional[bool] = None,\n",
    "        output_attentions: Optional[bool] = None,\n",
    "        output_hidden_states: Optional[bool] = None,\n",
    "        return_dict: Optional[bool] = None,\n",
    "    ) -> Union[Tuple, CausalLMOutputWithPast]:\n",
    "        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n",
    "        output_hidden_states = (\n",
    "            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n",
    "        )\n",
    "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
    "\n",
    "        # decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\n",
    "        outputs = self.model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            position_ids=position_ids,\n",
    "            past_key_values=past_key_values,\n",
    "            inputs_embeds=inputs_embeds,\n",
    "            use_cache=use_cache,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "            return_dict=return_dict,\n",
    "        )\n",
    "\n",
    "        hidden_states = outputs[0]\n",
    "\n",
    "        # TODO: maybe this should actually take the model dtype if I can get it\n",
    "        # Also, should I edit every token?\n",
    "        # torch.Size([1, 11, 4096])\n",
    "        # torch.Size([1, 1, 4096])\n",
    "        # Hack to only edit the first time through\n",
    "        if self.edit_weights is not None and self.edit_count == 0:\n",
    "            # edit_norm = self.edit / torch.norm(self.edit)\n",
    "            # # TODO: wait...do I want the final token or the first token?\n",
    "            # projection = edit_norm * torch.dot(hidden_states[:,-1,:].squeeze(0), edit_norm)\n",
    "            # print(projection.shape)\n",
    "            # hidden_states = hidden_states - projection\n",
    "            # print(hidden_states.shape)\n",
    "            # # TODO: renormalize?\n",
    "\n",
    "            '''\n",
    "            This is a crude edit that seems to work if I just use the probe weights\n",
    "        \n",
    "            unit_edit = self.edit/torch.norm(self.edit)\n",
    "            new_hidden_states = []\n",
    "            for token in hidden_states[0,:,:]:\n",
    "                new_hidden_states.append(token - torch.dot(token, unit_edit) * unit_edit)\n",
    "            new_hidden_states = torch.stack(new_hidden_states).unsqueeze(0)\n",
    "\n",
    "            hidden_states = new_hidden_states\n",
    "            '''\n",
    "            weights_normalized = self.edit_weights/torch.norm(self.edit_weights)\n",
    "            # print(\"self.edit_weights: \", self.edit_weights)\n",
    "            print(\"torch.norm(self.edit_weights): \", torch.norm(self.edit_weights).item())\n",
    "            # print(\"weights_normalized: \", weights_normalized)\n",
    "            # TODO: vectorize this — doing with a for loop to prevent weird projection behavior\n",
    "            new_hidden_states = []\n",
    "            for token in hidden_states[0,:,:]:\n",
    "                distance = torch.abs(torch.dot(self.edit_weights, token) + self.edit_bias)/torch.norm(self.edit_weights)\n",
    "                # print(\"distance: \", distance)\n",
    "                edit_distance = self.edit_scalar * torch.sign(distance) * distance * weights_normalized\n",
    "                # print(\"edit_distance norm: \", torch.norm(edit_distance))\n",
    "                new_hidden_state = token - edit_distance\n",
    "                # print(\"new_hidden_state norm: \", torch.norm(new_hidden_state).item())\n",
    "                new_distance = (torch.dot(self.edit_weights, new_hidden_state) + self.edit_bias)/torch.norm(self.edit_weights)\n",
    "                if new_distance != float('inf') and new_distance != -float('inf'):\n",
    "                    print(\"new_distance: \", new_distance.item())\n",
    "                # print(f\"norm diff: \", (torch.norm(token)-torch.norm(new_hidden_state)).item())\n",
    "                # Rescale new_hidden_state to have the same norm as the original token\n",
    "                if torch.norm(new_hidden_state) != 0:\n",
    "                    new_hidden_state_rescaled = new_hidden_state * (torch.norm(token) / torch.norm(new_hidden_state))\n",
    "                else:\n",
    "                    new_hidden_state_rescaled = new_hidden_state\n",
    "                new_hidden_states.append(new_hidden_state_rescaled)\n",
    "                # print(\"Orthogonal? \", torch.dot(new_hidden_state, weights_normalized))\n",
    "            hidden_states = torch.stack(new_hidden_states, dim=0).unsqueeze(0)\n",
    "            self.edit_count += 1\n",
    "            # print(\"new_hidden_states shape: \", new_hidden_states.shape)\n",
    "\n",
    "        if self.config.pretraining_tp > 1:\n",
    "            lm_head_slices = self.lm_head.weight.split(self.vocab_size // self.config.pretraining_tp, dim=0)\n",
    "            logits = [F.linear(hidden_states, lm_head_slices[i]) for i in range(self.config.pretraining_tp)]\n",
    "            logits = torch.cat(logits, dim=-1)\n",
    "        else:\n",
    "            logits = self.lm_head(hidden_states)\n",
    "        logits = logits.float()\n",
    "\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            # Shift so that tokens < n predict n\n",
    "            shift_logits = logits[..., :-1, :].contiguous()\n",
    "            shift_labels = labels[..., 1:].contiguous()\n",
    "            # Flatten the tokens\n",
    "            loss_fct = CrossEntropyLoss()\n",
    "            shift_logits = shift_logits.view(-1, self.config.vocab_size)\n",
    "            shift_labels = shift_labels.view(-1)\n",
    "            # Enable model parallelism\n",
    "            shift_labels = shift_labels.to(shift_logits.device)\n",
    "            loss = loss_fct(shift_logits, shift_labels)\n",
    "\n",
    "        if not return_dict:\n",
    "            output = (logits,) + outputs[1:]\n",
    "            return (loss,) + output if loss is not None else output\n",
    "\n",
    "        return CausalLMOutputWithPast(\n",
    "            loss=loss,\n",
    "            logits=logits,\n",
    "            past_key_values=outputs.past_key_values,\n",
    "            hidden_states=outputs.hidden_states,\n",
    "            attentions=outputs.attentions,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: Agriculture\n",
      "1: Alaska\n",
      "2: Alchemy\n",
      "3: Algae\n",
      "4: American football\n",
      "5: Amphibian\n",
      "6: Anarchism\n",
      "7: Animation\n",
      "8: Anthropology\n",
      "9: Appellate court\n",
      "10: Astronomer\n",
      "11: Basketball\n"
     ]
    }
   ],
   "source": [
    "for i, lbl in enumerate(label_encoder_title.classes_):\n",
    "    print(f\"{i}: {lbl}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.forward = forward.__get__(model, EditableLlama)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"I hate authority and my favorite form of government is\"\n",
    "input_ids = tokenizer(prompt, return_tensors='pt').input_ids.to(device)\n",
    "probe_idx = 6\n",
    "edit_scalar = 14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"I love going down to the park and playing some pickup \"\n",
    "input_ids = tokenizer(prompt, return_tensors='pt').input_ids.to(device)\n",
    "probe_idx = 11\n",
    "edit_scalar = 27.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"As a kid I was really interested in looking at the stars. When I grew up I wanted to be \"\n",
    "input_ids = tokenizer(prompt, return_tensors='pt').input_ids.to(device)\n",
    "probe_idx = 11\n",
    "edit_scalar = 27.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [],
   "source": [
    "edit_weights = 5 * torch.rand(4096) # random noise\n",
    "edit_weights = None\n",
    "edit_weights = probe.fc1.weight.data[probe_idx]\n",
    "edit_bias = probe.fc1.bias[probe_idx]\n",
    "\n",
    "edit_weights = edit_weights.half().to(device)\n",
    "edit_bias = edit_bias.half().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.norm(self.edit_weights):  2262.0\n",
      "new_distance:  -8.078125\n",
      "norm diff:  -0.375\n",
      "new_distance:  -1.3388671875\n",
      "norm diff:  -0.0625\n",
      "new_distance:  -3.26171875\n",
      "norm diff:  -0.0625\n",
      "new_distance:  -11.4921875\n",
      "norm diff:  -0.4375\n",
      "new_distance:  -17.453125\n",
      "norm diff:  -1.0625\n",
      "new_distance:  -14.6015625\n",
      "norm diff:  -0.75\n",
      "new_distance:  -4.1875\n",
      "norm diff:  -0.0625\n",
      "new_distance:  -17.734375\n",
      "norm diff:  -1.0625\n",
      "new_distance:  -10.4375\n",
      "norm diff:  -0.375\n",
      "new_distance:  -15.0390625\n",
      "norm diff:  -0.75\n",
      "new_distance:  -13.1171875\n",
      "norm diff:  -0.5625\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"I hate authority and my favorite form of government is anarchy.\\nI'm a big fan of the idea of a free society, but I'm not sure that anarchy is the way to get there.\\nI think that an\""
      ]
     },
     "execution_count": 287,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = model.generate_with_edit(input_ids, edit_weights=edit, edit_bias=edit_bias, edit_scalar=edit_scalar, max_length=50, num_return_sequences=1, output_hidden_states=False)\n",
    "generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "generated_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I love going down to the park and playing some pickup 5-on-5 basketball. I'm not the best player on the court, but I'm definitely not the worst. I'm a solid player, and I\""
      ]
     },
     "execution_count": 261,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = model.generate(input_ids, max_length=50, num_return_sequences=1, output_hidden_states=False)\n",
    "generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "generated_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'LeBron James is great at the sport of 1-on-1 basketball. He has proven that he can dominate the game, even against the best players in the world. But when it comes to team basketball, he has yet'"
      ]
     },
     "execution_count": 228,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Notes:\n",
    "# If you just add the probe weights multiplied by 100, you get the probe word repeated ie \"animation\", \"anth\", \"Alaska\"\n",
    "# Interestingly, anarchy seems to give \"libert\" (as in \"liberty\") so these are a similar direction\n",
    "# Subtracting gives \"Dictator\"...for Alaska\n",
    "# Gives \"Science\" for \"Anarchy\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "wiki-probes",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
