{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# handle gpu leakage issue on DSI cluster\n",
    "# ideally we shouldn't have to do this but leaving it for now in case the issue pops up again\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Optional, Tuple, Union\n",
    "\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, LlamaForCausalLM, LlamaTokenizer\n",
    "from transformers.modeling_outputs import CausalLMOutputWithPast\n",
    "\n",
    "import scripts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Confirm GPUs are working properly and set default device explicitly\n",
    "device = \"cuda:0\"\n",
    "torch.cuda.device_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## References ##\n",
    "# Huggingface docs: https://huggingface.co/docs/transformers/main/model_doc/llama#transformers.LlamaForCausalLM \n",
    "\n",
    "## modeling_llama.py ##\n",
    "# LlamaForCausalLM » https://github.com/huggingface/transformers/blob/main/src/transformers/models/llama/modeling_llama.py#L989 \n",
    "# forward » https://github.com/huggingface/transformers/blob/main/src/transformers/models/llama/modeling_llama.py#L989 \n",
    "\n",
    "## utils.py ##\n",
    "# GenerationMixin » https://github.com/huggingface/transformers/blob/main/src/transformers/generation/utils.py#L571\n",
    "# generate » https://github.com/huggingface/transformers/blob/main/src/transformers/generation/utils.py#L1432\n",
    "# greedy_search » https://github.com/huggingface/transformers/blob/main/src/transformers/generation/utils.py#L2447 \n",
    "# actual forward pass » https://github.com/huggingface/transformers/blob/main/src/transformers/generation/utils.py#L2614 \n",
    "\n",
    "class EditableLlama(LlamaForCausalLM):\n",
    "    def __init__(self, config, *args, **kwargs):\n",
    "        super().__init__(config, *args, **kwargs)\n",
    "        # self.edit_weights = None\n",
    "        # self.edit_bias = None\n",
    "        # self.edit_scalar = None\n",
    "        # self.edit_count = 0\n",
    "        self.edit_probe = None\n",
    "        self.edit_scalare = None\n",
    "\n",
    "    # def generate_with_edit(self, inputs, edit_weights=None, edit_bias=None, edit_scalar=None, **generate_kwargs):\n",
    "        # self.edit_weights = edit_weights\n",
    "        # self.edit_bias = edit_bias\n",
    "        # self.edit_scalar = edit_scalar\n",
    "        # self.edit_count = 0\n",
    "    def generate_with_edit(self, inputs, edit_probe=None, edit_scalar=None, **generate_kwargs):\n",
    "        self.edit_probe = edit_probe\n",
    "        self.edit_scalar = edit_scalar\n",
    "        return self.generate(inputs, **generate_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_DIR = \"/net/projects/veitch/LLMs/\"\n",
    "MODEL = \"llama1-based-models/alpaca-7b\" # or llama2-based-models/llama2-hf/Llama-2-7b-chat-hf\n",
    "MODEL_PATH = MODEL_DIR + MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Clean this up\n",
    "# This cell is for doing ROME edits\n",
    "# ROME model and tokenizer\n",
    "# MODEL_NAME = \"gpt2-xl\"\n",
    "# model = AutoModelForCausalLM.from_pretrained(MODEL_NAME).to(device)\n",
    "# tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "# tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This means that tokens that come after special tokens will not be properly handled. We recommend you to read the related pull request available at https://github.com/huggingface/transformers/pull/24565\n",
      "normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = LlamaTokenizer.from_pretrained(MODEL_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a07380b352b4c12a2982fb9d2867c1a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# model = LlamaForCausalLM.from_pretrained(MODEL_PATH, low_cpu_mem_usage=True, torch_dtype=torch.float16, device_map=\"auto\").to(device)\n",
    "model = EditableLlama.from_pretrained(MODEL_PATH, low_cpu_mem_usage=True, torch_dtype=torch.float16, device_map=\"auto\").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "LABELS = \"title\" # choices are \"title\" and \"categories\"\n",
    "DATA_FOLDER = \"data/\"\n",
    "TRAIN_FILEPATH = DATA_FOLDER + \"train-12-articles.csv\"\n",
    "VAL_FILEPATH = DATA_FOLDER + \"val-12-articles.csv\"\n",
    "PROBE_TYPE = \"linear\" # choices are \"linear\" and \"mlp\"\n",
    "LAYER = None\n",
    "EPOCHS = 50\n",
    "BATCH_SIZE = 4\n",
    "AGGREGATION = \"max\" # choices are \"max\" or \"mean\"\n",
    "PRINT_PROGRESS = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, label_encoder_title, label_encoder_cats = scripts.get_hidden_states(TRAIN_FILEPATH, model, tokenizer, device)\n",
    "val_data, _, _ = scripts.get_hidden_states(VAL_FILEPATH, model, tokenizer, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to use entire flattened activations use layer = None\n",
    "# to use final layer, use layer = -1\n",
    "train_dataset = scripts.create_dataset(*train_data, layer=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dataset = scripts.create_dataset(*val_data, layer=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([135168])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_dataset.Xs[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Training][5] loss: 2307.721\n",
      "[Validation][5] loss: 12.711\n",
      "[Validation]5 accuracy: 0.750\n",
      "[Training][10] loss: 3891.391\n",
      "[Validation][10] loss: 24.403\n",
      "[Validation]10 accuracy: 0.778\n",
      "[Training][15] loss: 4837.555\n",
      "[Validation][15] loss: 20.106\n",
      "[Validation]15 accuracy: 0.833\n",
      "[Training][20] loss: 5007.014\n",
      "[Validation][20] loss: 10.569\n",
      "[Validation]20 accuracy: 0.917\n",
      "[Training][25] loss: 4929.658\n",
      "[Validation][25] loss: 10.406\n",
      "[Validation]25 accuracy: 0.917\n",
      "[Training][30] loss: 4853.561\n",
      "[Validation][30] loss: 10.245\n",
      "[Validation]30 accuracy: 0.917\n",
      "[Training][35] loss: 4778.507\n",
      "[Validation][35] loss: 10.087\n",
      "[Validation]35 accuracy: 0.917\n",
      "[Training][40] loss: 4704.619\n",
      "[Validation][40] loss: 9.931\n",
      "[Validation]40 accuracy: 0.917\n",
      "[Training][45] loss: 4632.045\n",
      "[Validation][45] loss: 9.777\n",
      "[Validation]45 accuracy: 0.917\n",
      "[Training][50] loss: 4560.499\n",
      "[Validation][50] loss: 9.626\n",
      "[Validation]50 accuracy: 0.917\n"
     ]
    }
   ],
   "source": [
    "probe, _ = scripts.train_handler(train_dataset, val_dataset, label_encoder_title, probe_type=PROBE_TYPE, labels=LABELS, batch_size=BATCH_SIZE, epochs=EPOCHS, print_progress=PRINT_PROGRESS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save probe to load elsewhere\n",
    "WEIGHTS_FOLDER = \"model-weights/\"\n",
    "PROBE_FILE = \"probe_weights.pt\"\n",
    "# TODO: maybe put together an actual config setup\n",
    "torch.save(probe.state_dict(), WEIGHTS_FOLDER + PROBE_FILE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try Editing With Linear Probe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probe.fc1.weight.data.shape, probe.fc1.bias.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(\n",
    "        self,\n",
    "        input_ids: torch.LongTensor = None,\n",
    "        attention_mask: Optional[torch.Tensor] = None,\n",
    "        position_ids: Optional[torch.LongTensor] = None,\n",
    "        past_key_values: Optional[List[torch.FloatTensor]] = None,\n",
    "        inputs_embeds: Optional[torch.FloatTensor] = None,\n",
    "        labels: Optional[torch.LongTensor] = None,\n",
    "        use_cache: Optional[bool] = None,\n",
    "        output_attentions: Optional[bool] = None,\n",
    "        output_hidden_states: Optional[bool] = None,\n",
    "        return_dict: Optional[bool] = None,\n",
    "    ) -> Union[Tuple, CausalLMOutputWithPast]:\n",
    "        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n",
    "        output_hidden_states = (\n",
    "            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n",
    "        )\n",
    "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
    "\n",
    "        # decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\n",
    "        outputs = self.model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            position_ids=position_ids,\n",
    "            past_key_values=past_key_values,\n",
    "            inputs_embeds=inputs_embeds,\n",
    "            use_cache=use_cache,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "            return_dict=return_dict,\n",
    "        )\n",
    "\n",
    "        hidden_states = outputs[0]\n",
    "\n",
    "        # TODO: maybe this should actually take the model dtype if I can get it\n",
    "        # Also, should I edit every token?\n",
    "        # torch.Size([1, 11, 4096])\n",
    "        # torch.Size([1, 1, 4096])\n",
    "        # # Hack to only edit the first time through\n",
    "        # if self.edit_weights is not None and self.edit_count == 0:\n",
    "        # TODO: should do this so that we edit if the probe is classifying as \"yes\"\n",
    "        if self.edit_weights is not None:\n",
    "            # edit_norm = self.edit / torch.norm(self.edit)\n",
    "            # # TODO: wait...do I want the final token or the first token?\n",
    "            # projection = edit_norm * torch.dot(hidden_states[:,-1,:].squeeze(0), edit_norm)\n",
    "            # print(projection.shape)\n",
    "            # hidden_states = hidden_states - projection\n",
    "            # print(hidden_states.shape)\n",
    "            # # TODO: renormalize?\n",
    "\n",
    "            '''\n",
    "            This is a crude edit that seems to work if I just use the probe weights\n",
    "        \n",
    "            unit_edit = self.edit/torch.norm(self.edit)\n",
    "            new_hidden_states = []\n",
    "            for token in hidden_states[0,:,:]:\n",
    "                new_hidden_states.append(token - torch.dot(token, unit_edit) * unit_edit)\n",
    "            new_hidden_states = torch.stack(new_hidden_states).unsqueeze(0)\n",
    "\n",
    "            hidden_states = new_hidden_states\n",
    "            '''\n",
    "            weights_normalized = self.edit_weights/torch.norm(self.edit_weights)\n",
    "            # print(\"self.edit_weights: \", self.edit_weights)\n",
    "            print(\"torch.norm(self.edit_weights): \", torch.norm(self.edit_weights).item())\n",
    "            # print(\"weights_normalized: \", weights_normalized)\n",
    "            # TODO: vectorize this — doing with a for loop to prevent weird projection behavior\n",
    "            new_hidden_states = []\n",
    "            for token in hidden_states[0,:,:]:\n",
    "                distance = torch.abs(torch.dot(self.edit_weights, token) + self.edit_bias)/torch.norm(self.edit_weights)\n",
    "                # print(\"distance: \", distance)\n",
    "                edit_distance = self.edit_scalar * torch.sign(distance) * distance * weights_normalized\n",
    "                # print(\"edit_distance norm: \", torch.norm(edit_distance))\n",
    "                new_hidden_state = token - edit_distance\n",
    "                # print(\"new_hidden_state norm: \", torch.norm(new_hidden_state).item())\n",
    "                new_distance = (torch.dot(self.edit_weights, new_hidden_state) + self.edit_bias)/torch.norm(self.edit_weights)\n",
    "                # if new_distance != float('inf') and new_distance != -float('inf'):\n",
    "                #     print(\"new_distance: \", new_distance.item())\n",
    "                # print(f\"norm diff: \", (torch.norm(token)-torch.norm(new_hidden_state)).item())\n",
    "                # Rescale new_hidden_state to have the same norm as the original token\n",
    "                if torch.norm(new_hidden_state) != 0:\n",
    "                    new_hidden_state_rescaled = new_hidden_state * (torch.norm(token) / torch.norm(new_hidden_state))\n",
    "                else:\n",
    "                    new_hidden_state_rescaled = new_hidden_state\n",
    "                new_hidden_states.append(new_hidden_state_rescaled)\n",
    "                # print(\"Orthogonal? \", torch.dot(new_hidden_state, weights_normalized))\n",
    "            hidden_states = torch.stack(new_hidden_states, dim=0).unsqueeze(0)\n",
    "            self.edit_count += 1\n",
    "            # print(\"new_hidden_states shape: \", new_hidden_states.shape)\n",
    "\n",
    "        if self.config.pretraining_tp > 1:\n",
    "            lm_head_slices = self.lm_head.weight.split(self.vocab_size // self.config.pretraining_tp, dim=0)\n",
    "            logits = [F.linear(hidden_states, lm_head_slices[i]) for i in range(self.config.pretraining_tp)]\n",
    "            logits = torch.cat(logits, dim=-1)\n",
    "        else:\n",
    "            logits = self.lm_head(hidden_states)\n",
    "        logits = logits.float()\n",
    "\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            # Shift so that tokens < n predict n\n",
    "            shift_logits = logits[..., :-1, :].contiguous()\n",
    "            shift_labels = labels[..., 1:].contiguous()\n",
    "            # Flatten the tokens\n",
    "            loss_fct = CrossEntropyLoss()\n",
    "            shift_logits = shift_logits.view(-1, self.config.vocab_size)\n",
    "            shift_labels = shift_labels.view(-1)\n",
    "            # Enable model parallelism\n",
    "            shift_labels = shift_labels.to(shift_logits.device)\n",
    "            loss = loss_fct(shift_logits, shift_labels)\n",
    "\n",
    "        if not return_dict:\n",
    "            output = (logits,) + outputs[1:]\n",
    "            return (loss,) + output if loss is not None else output\n",
    "\n",
    "        return CausalLMOutputWithPast(\n",
    "            loss=loss,\n",
    "            logits=logits,\n",
    "            past_key_values=outputs.past_key_values,\n",
    "            hidden_states=outputs.hidden_states,\n",
    "            attentions=outputs.attentions,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, lbl in enumerate(label_encoder_title.classes_):\n",
    "    print(f\"{i}: {lbl}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.forward = forward.__get__(model, EditableLlama)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"I hate authority and my favorite form of government is\"\n",
    "input_ids = tokenizer(prompt, return_tensors='pt').input_ids.to(device)\n",
    "probe_idx = 6\n",
    "edit_scalar = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"I love going down to the park and playing some pickup \"\n",
    "input_ids = tokenizer(prompt, return_tensors='pt').input_ids.to(device)\n",
    "probe_idx = 11\n",
    "edit_scalar = 13\n",
    "# edit_scalar = 27.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"As a kid I was really interested in looking at the stars. When I grew up I wanted to be \"\n",
    "input_ids = tokenizer(prompt, return_tensors='pt').input_ids.to(device)\n",
    "probe_idx = 11\n",
    "edit_scalar = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edit_weights = 5 * torch.rand(4096) # random noise\n",
    "edit_weights = None\n",
    "edit_weights = probe.fc1.weight.data[probe_idx]\n",
    "edit_bias = probe.fc1.bias[probe_idx]\n",
    "\n",
    "edit_weights = edit_weights.half().to(device)\n",
    "edit_bias = edit_bias.half().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = model.generate_with_edit(input_ids, edit_weights=edit_weights, edit_bias=edit_bias, edit_scalar=edit_scalar, max_length=50, num_return_sequences=1, output_hidden_states=False)\n",
    "generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "generated_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = model.generate(input_ids, max_length=50, num_return_sequences=1, output_hidden_states=False)\n",
    "generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "generated_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Notes:\n",
    "# If you just add the probe weights multiplied by 100, you get the probe word repeated ie \"animation\", \"anth\", \"Alaska\"\n",
    "# Interestingly, anarchy seems to give \"libert\" (as in \"liberty\") so these are a similar direction\n",
    "# Subtracting gives \"Dictator\"...for Alaska\n",
    "# Gives \"Science\" for \"Anarchy\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "wiki-probes",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
